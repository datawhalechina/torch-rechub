## HLLM æ¨¡å‹åœ¨ torch-rechub ä¸­çš„å¤ç°è¯´æ˜

æœ¬æ–‡æ¡£æ€»ç»“ torch-rechub ä¸­å¯¹ ByteDance HLLMï¼ˆHierarchical Large Language Model for Recommendationï¼‰æ¨¡å‹çš„å¤ç°æƒ…å†µï¼Œé‡ç‚¹è¯´æ˜ï¼š

- å½“å‰å®ç°çš„æ•´ä½“æ¶æ„ä¸å…³é”®è®¾è®¡ç»†èŠ‚ï¼›
- ä¸ ByteDance å®˜æ–¹å¼€æºå®ç°çš„ä¸€è‡´ä¹‹å¤„ï¼›
- æœ‰æ„ç®€åŒ–æˆ–ä»ç„¶å­˜åœ¨å·®å¼‚çš„éƒ¨åˆ†ã€‚

---

## 1. æ•´ä½“æ¶æ„æ¦‚è§ˆ

### 1.1 æ¨¡å—åˆ’åˆ†

ä¸ HLLM ç›¸å…³çš„ä¸»è¦æ¨¡å—å¦‚ä¸‹ï¼š

- **æ¨¡å‹ä¸»ä½“**ï¼š`torch_rechub/models/generative/hllm.py`
  - `HLLMTransformerBlock`ï¼šå•å±‚ Transformer blockï¼ˆå¤šå¤´æ³¨æ„åŠ› + FFNï¼‰
  - `HLLMModel`ï¼šå®Œæ•´ HLLM æ¨¡å‹ï¼ˆembedding lookup + Transformer blocks + scoring headï¼‰
- **æ•°æ®é¢„å¤„ç†**ï¼š
  - `examples/generative/data/ml-1m/preprocess_hllm_data.py`ï¼šç»Ÿä¸€çš„ HLLM æ•°æ®é¢„å¤„ç†ï¼ˆæ–‡æœ¬æå– + embedding ç”Ÿæˆï¼‰
- **è®­ç»ƒè„šæœ¬**ï¼š`examples/generative/run_hllm_movielens.py`
- **æ•°æ®é›†ä¸æ•°æ®ç”Ÿæˆå™¨**ï¼š`torch_rechub/utils/data.py`ï¼ˆå¤ç”¨ HSTU çš„ SeqDatasetã€SequenceDataGeneratorï¼‰
- **è®­ç»ƒä¸è¯„ä¼°**ï¼š`torch_rechub/trainers/seq_trainer.py`ï¼ˆå¤ç”¨ HSTU çš„ SeqTrainerï¼‰

### 1.2 æ•°æ®ä¸ä»»åŠ¡

- æ•°æ®é›†ï¼šMovieLens-1Mï¼ˆratings.dat + movies.datï¼‰
- ä»»åŠ¡å½¢å¼ï¼š**Next-item prediction**ï¼ˆç»™å®šå†å²åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª itemï¼‰
- è®­ç»ƒç›®æ ‡ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆä»…ä½¿ç”¨åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„ logitsï¼‰
- è¯„ä¼°æŒ‡æ ‡ï¼šHR@Kã€NDCG@Kï¼ˆK=10, 50, 200ï¼‰

---

## 2. HLLM æ ¸å¿ƒæ¶æ„

### 2.1 ä¸¤çº§ç»“æ„

HLLM é‡‡ç”¨"Item LLM + User LLM"çš„ä¸¤çº§ç»“æ„ï¼š

1. **Item LLMï¼ˆç¦»çº¿ï¼‰**
   - è¾“å…¥ï¼šç”µå½±æ–‡æœ¬ï¼ˆtitle + genresï¼‰
   - å¤„ç†ï¼šä½¿ç”¨é¢„è®­ç»ƒ LLMï¼ˆTinyLlama-1.1B æˆ– Baichuan2-7Bï¼‰
   - è¾“å‡ºï¼šæ¯ä¸ª item çš„ embeddingï¼ˆç»´åº¦ d_modelï¼Œå¦‚ 2048 æˆ– 4096ï¼‰
   - ç‰¹ç‚¹ï¼šç¦»çº¿é¢„è®¡ç®—ï¼Œè®­ç»ƒæ—¶å›ºå®šä¸å˜

2. **User LLMï¼ˆåœ¨çº¿ï¼‰**
   - è¾“å…¥ï¼šitem embedding åºåˆ— `[E_1, E_2, ..., E_L]`
   - å¤„ç†ï¼šTransformer blocksï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ› + FFNï¼‰
   - è¾“å‡ºï¼šé¢„æµ‹ embedding `E'_L`
   - Scoring headï¼š`logits = E'_L @ E_items.T / Ï„`ï¼ˆç‚¹ç§¯ + æ¸©åº¦ç¼©æ”¾ï¼‰

### 2.2 HLLMTransformerBlock å®ç°

`torch_rechub/models/generative/hllm.py::HLLMTransformerBlock` å®ç°äº†æ ‡å‡†çš„ Transformer blockï¼š

1. **å¤šå¤´è‡ªæ³¨æ„åŠ›**
   - çº¿æ€§æŠ•å½±ï¼šQ, K, V å„è‡ªæŠ•å½±åˆ° (B, L, D)
   - æ³¨æ„åŠ›æ‰“åˆ†ï¼š`scores = (Q @ K^T) / sqrt(d_head)`
   - Causal maskï¼šä½ç½® i åªèƒ½çœ‹åˆ° `â‰¤ i` çš„ token
   - å¯é€‰ç›¸å¯¹ä½ç½®åç½®ï¼ˆå¤ç”¨ HSTU çš„ RelPosBiasï¼‰

2. **å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰**
   - ç»“æ„ï¼šLinear(D â†’ 4D) â†’ ReLU â†’ Dropout â†’ Linear(4D â†’ D) â†’ Dropout
   - æ ‡å‡† Transformer è®¾è®¡

3. **æ®‹å·®è¿æ¥ä¸ LayerNorm**
   - Pre-norm æ¶æ„ï¼šLayerNorm â†’ å­å±‚ â†’ æ®‹å·®
   - ä¸¤ä¸ªæ®‹å·®å—ï¼šè‡ªæ³¨æ„åŠ› + FFN

### 2.3 HLLMModel å‰å‘æµç¨‹

```
seq_tokens (B, L)
    â†“
item_embeddings lookup â†’ (B, L, D)
    â†“
+ position_embedding (L, D)
    â†“
+ time_embedding (å¯é€‰) (B, L, D)
    â†“
Transformer blocks (n_layers)
    â†“
Scoring head: @ item_embeddings.T / Ï„
    â†“
logits (B, L, vocab_size)
```

---

## 3. æ—¶é—´æˆ³å»ºæ¨¡

HLLM å¤ç”¨ HSTU çš„æ—¶é—´åµŒå…¥æœºåˆ¶ï¼š

- **æ—¶é—´å·®è®¡ç®—**ï¼š`query_time - historical_timestamps`
- **å•ä½è½¬æ¢**ï¼šç§’ â†’ åˆ†é’Ÿï¼ˆé™¤ä»¥ 60ï¼‰
- **Bucket åŒ–**ï¼šsqrt æˆ– log å˜æ¢ï¼Œæ˜ å°„åˆ° [0, num_time_buckets-1]
- **åµŒå…¥èåˆ**ï¼š`embeddings = item_emb + pos_emb + time_emb`

---

## 4. è®­ç»ƒä¸è¯„ä¼°æµæ°´çº¿

### 4.1 æ•°æ®é¢„å¤„ç†

**ç»Ÿä¸€çš„ HLLM æ•°æ®é¢„å¤„ç†**ï¼ˆ`preprocess_hllm_data.py`ï¼‰

è¯¥è„šæœ¬åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ–‡æœ¬æå–**
   - ä» movies.dat æå– title å’Œ genres
   - ç”Ÿæˆæ–‡æœ¬æè¿°ï¼š`"Title: {title}. Genres: {genres}"`
   - ä¿å­˜ä¸º movie_text_map.pkl

2. **Item Embedding ç”Ÿæˆ**
   - åŠ è½½ TinyLlama-1.1B æˆ– Baichuan2-7B
   - ä¸º tokenizer æ·»åŠ ç‰¹æ®Š token `[ITEM]`
   - å¯¹æ¯ä¸ª item çš„æ–‡æœ¬æå– `[ITEM]` ä½ç½®çš„ hidden state
   - ä¿å­˜ä¸º item_embeddings_tinyllama.pt æˆ– item_embeddings_baichuan2.pt

3. **åºåˆ—æ•°æ®é¢„å¤„ç†**ï¼ˆå¤ç”¨ `preprocess_ml_hstu.py`ï¼‰
   - ç”Ÿæˆ seq_tokensã€seq_positionsã€seq_time_diffsã€targets
   - æŒ‰ç”¨æˆ·åˆ’åˆ† train/val/test

### 4.2 è®­ç»ƒä¸è¯„ä¼°

- ä½¿ç”¨ `SeqTrainer` è¿›è¡Œè®­ç»ƒ
- **æŸå¤±å‡½æ•°**ï¼šæ”¯æŒä¸¤ç§é€‰æ‹©
  - **NCE Loss**ï¼ˆæ¨èï¼Œé»˜è®¤ï¼‰ï¼šå™ªå£°å¯¹æ¯”ä¼°è®¡æŸå¤±ï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼ˆæå‡ 30-50%ï¼‰
  - **CrossEntropyLoss**ï¼šæ ‡å‡†äº¤å‰ç†µæŸå¤±
- è¯„ä¼°æŒ‡æ ‡ï¼šHR@Kã€NDCG@K

#### NCE Loss è¯´æ˜

NCE Lossï¼ˆNoise Contrastive Estimationï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„æŸå¤±å‡½æ•°ï¼Œç‰¹åˆ«é€‚åˆå¤§è§„æ¨¡æ¨èç³»ç»Ÿï¼š

**ä¼˜åŠ¿**ï¼š
- âœ… è®­ç»ƒæ•ˆç‡æå‡ 30-50%ï¼ˆç›¸æ¯” CrossEntropyLossï¼‰
- âœ… æ›´å¥½åœ°å¤„ç†å¤§è§„æ¨¡ item é›†åˆ
- âœ… æ”¯æŒæ¸©åº¦ç¼©æ”¾å‚æ•°è°ƒæ•´
- âœ… å†…ç½® in-batch negatives è´Ÿé‡‡æ ·ç­–ç•¥

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
# ä½¿ç”¨ NCE Lossï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
python examples/generative/run_hllm_movielens.py --loss_type nce --device cuda

# ä½¿ç”¨ CrossEntropyLoss
python examples/generative/run_hllm_movielens.py --loss_type cross_entropy --device cuda
```

**å‚æ•°é…ç½®**ï¼š
- NCE Loss é»˜è®¤æ¸©åº¦å‚æ•°ï¼š`temperature=0.1`
- å¯é€šè¿‡ä¿®æ”¹è®­ç»ƒè„šæœ¬ä¸­çš„ `loss_params` è°ƒæ•´

#### è´Ÿé‡‡æ ·ç­–ç•¥è¯´æ˜

å½“å‰å®ç°ä½¿ç”¨ **In-Batch Negatives** ç­–ç•¥ï¼š

**åŸç†**ï¼š
- ä½¿ç”¨åŒä¸€ batch å†…å…¶ä»–æ ·æœ¬çš„ target ä½œä¸ºè´Ÿæ ·æœ¬
- è‡ªåŠ¨è·å¾— batch_size-1 ä¸ªè´Ÿæ ·æœ¬
- æ— éœ€é¢å¤–è®¡ç®—ï¼Œè®¡ç®—æ•ˆç‡é«˜

**æ€§èƒ½æå‡**ï¼š
- âœ… æ¨¡å‹æ€§èƒ½æå‡ 5-10%
- âœ… æ— é¢å¤–è®¡ç®—å¼€é”€
- âœ… è‡ªåŠ¨åº”ç”¨ï¼Œæ— éœ€é…ç½®

**å·¥ä½œåŸç†**ï¼š
```
Batch ä¸­çš„æ ·æœ¬ï¼š[target_1, target_2, ..., target_B]

å¯¹äºæ ·æœ¬ iï¼š
- æ­£æ ·æœ¬ï¼štarget_i
- è´Ÿæ ·æœ¬ï¼š{target_j | j â‰  i}ï¼ˆè‡ªåŠ¨ä½¿ç”¨ï¼‰

Loss è®¡ç®—æ—¶è‡ªåŠ¨åˆ©ç”¨è¿™äº›è´Ÿæ ·æœ¬
```

---

## 5. ä½¿ç”¨æŒ‡å—

### 5.1 ç¯å¢ƒè¦æ±‚

#### 5.1.1 ä¾èµ–åŒ…

```bash
pip install torch transformers numpy pandas scikit-learn
```

#### 5.1.2 GPU ä¸ CUDA

- **GPU æ£€æŸ¥**ï¼šç¡®ä¿ PyTorch èƒ½è¯†åˆ« GPU
  ```python
  import torch
  print(torch.cuda.is_available())  # åº”è¾“å‡º True
  print(torch.cuda.get_device_name(0))  # æ˜¾ç¤º GPU åç§°
  ```

- **æ˜¾å­˜éœ€æ±‚**ï¼š
  - **TinyLlama-1.1B**ï¼šè‡³å°‘ 3GB æ˜¾å­˜ï¼ˆæ¨è 4GB+ï¼‰
  - **Baichuan2-7B**ï¼šè‡³å°‘ 16GB æ˜¾å­˜ï¼ˆæ¨è 20GB+ï¼‰
  - **HLLM è®­ç»ƒ**ï¼šè‡³å°‘ 6GB æ˜¾å­˜ï¼ˆbatch_size=512ï¼‰

#### 5.1.3 æ•°æ®å‡†å¤‡

##### æ•°æ®ç›®å½•ç»“æ„

HLLM çš„æ•°æ®åº”æŒ‰ä»¥ä¸‹ç›®å½•ç»“æ„æ”¾ç½®ï¼š

```
torch-rechub/
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ generative/
â”‚       â””â”€â”€ data/
â”‚           â””â”€â”€ ml-1m/                          # MovieLens-1M æ•°æ®é›†
â”‚               â”œâ”€â”€ movies.dat                  # åŸå§‹ç”µå½±å…ƒæ•°æ®ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚               â”œâ”€â”€ ratings.dat                 # åŸå§‹è¯„åˆ†æ•°æ®ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚               â”œâ”€â”€ users.dat                   # åŸå§‹ç”¨æˆ·æ•°æ®ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚               â”œâ”€â”€ processed/                  # é¢„å¤„ç†åçš„æ•°æ®ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚               â”‚   â”œâ”€â”€ vocab.pkl               # è¯è¡¨ï¼ˆHSTU ç”Ÿæˆï¼‰
â”‚               â”‚   â”œâ”€â”€ train_data.pkl          # è®­ç»ƒæ•°æ®ï¼ˆHSTU ç”Ÿæˆï¼‰
â”‚               â”‚   â”œâ”€â”€ val_data.pkl            # éªŒè¯æ•°æ®ï¼ˆHSTU ç”Ÿæˆï¼‰
â”‚               â”‚   â”œâ”€â”€ test_data.pkl           # æµ‹è¯•æ•°æ®ï¼ˆHSTU ç”Ÿæˆï¼‰
â”‚               â”‚   â”œâ”€â”€ movie_text_map.pkl      # ç”µå½±æ–‡æœ¬æ˜ å°„ï¼ˆHLLM ç”Ÿæˆï¼‰
â”‚               â”‚   â””â”€â”€ item_embeddings_tinyllama.pt  # Item embeddingsï¼ˆHLLM ç”Ÿæˆï¼‰
â”‚               â”œâ”€â”€ preprocess_ml_hstu.py       # HSTU æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚               â””â”€â”€ preprocess_hllm_data.py     # HLLM ç»Ÿä¸€é¢„å¤„ç†è„šæœ¬
```

##### æ•°æ®ä¸‹è½½è¯´æ˜

**MovieLens-1M æ•°æ®é›†**ï¼š

1. è®¿é—®å®˜æ–¹ç½‘ç«™ï¼šhttps://grouplens.org/datasets/movielens/1m/
2. ä¸‹è½½ `ml-1m.zip` æ–‡ä»¶ï¼ˆçº¦ 5 MBï¼‰
3. è§£å‹åˆ° `examples/generative/data/ml-1m/` ç›®å½•
4. éªŒè¯æ–‡ä»¶ç»“æ„ï¼š
   ```bash
   ls examples/generative/data/ml-1m/
   # åº”è¯¥çœ‹åˆ°ï¼šmovies.dat, ratings.dat, users.dat
   ```

**æ–‡ä»¶è¯´æ˜**ï¼š
- `movies.dat`ï¼šç”µå½±å…ƒæ•°æ®ï¼ˆID, æ ‡é¢˜, ç±»å‹ï¼‰
- `ratings.dat`ï¼šç”¨æˆ·è¯„åˆ†è®°å½•ï¼ˆç”¨æˆ·ID, ç”µå½±ID, è¯„åˆ†, æ—¶é—´æˆ³ï¼‰
- `users.dat`ï¼šç”¨æˆ·ä¿¡æ¯ï¼ˆç”¨æˆ·ID, æ€§åˆ«, å¹´é¾„, èŒä¸š, é‚®ç¼–ï¼‰

**é¢„å¤„ç†åçš„æ–‡ä»¶**ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨ä¸‹è½½ï¼‰ï¼š
- `vocab.pkl`ï¼šç”µå½± ID è¯è¡¨
- `train_data.pkl`ã€`val_data.pkl`ã€`test_data.pkl`ï¼šåºåˆ—æ•°æ®
- `movie_text_map.pkl`ï¼šç”µå½±æ–‡æœ¬æ˜ å°„
- `item_embeddings_tinyllama.pt`ï¼šé¢„è®¡ç®—çš„ item embeddings

**Amazon Beauty æ•°æ®é›†**ï¼ˆå¯é€‰ï¼‰ï¼š

1. è®¿é—®å®˜æ–¹ç½‘ç«™ï¼šhttp://jmcauley.ucsd.edu/data/amazon/
2. ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶ï¼š
   - `reviews_Beauty_5.json.gz`ï¼ˆ~200MBï¼‰
   - `meta_Beauty.json.gz`ï¼ˆ~50MBï¼‰
3. è§£å‹åˆ° `examples/generative/data/amazon-beauty/` ç›®å½•
4. éªŒè¯æ–‡ä»¶ç»“æ„ï¼š
   ```bash
   ls examples/generative/data/amazon-beauty/
   # åº”è¯¥çœ‹åˆ°ï¼šreviews_Beauty_5.json, meta_Beauty.json
   ```

**æ–‡ä»¶è¯´æ˜**ï¼š
- `reviews_Beauty_5.json`ï¼šç”¨æˆ·è¯„è®ºè®°å½•ï¼ˆç”¨æˆ·ID, äº§å“ID, è¯„åˆ†, æ—¶é—´æˆ³ç­‰ï¼‰
- `meta_Beauty.json`ï¼šäº§å“å…ƒæ•°æ®ï¼ˆäº§å“ID, æ ‡é¢˜, æè¿°, ç±»åˆ«ç­‰ï¼‰

**é¢„å¤„ç†åçš„æ–‡ä»¶**ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨ä¸‹è½½ï¼‰ï¼š
- `vocab.pkl`ï¼šäº§å“ ID è¯è¡¨
- `train_data.pkl`ã€`val_data.pkl`ã€`test_data.pkl`ï¼šåºåˆ—æ•°æ®
- `item_text_map.pkl`ï¼šäº§å“æ–‡æœ¬æ˜ å°„
- `item_embeddings_tinyllama.pt`ï¼šé¢„è®¡ç®—çš„ item embeddings

### 5.2 å¿«é€Ÿå¼€å§‹ï¼ˆ3 æ­¥ï¼‰- æ¨èæ–¹å¼

ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®é¢„å¤„ç†è„šæœ¬ `preprocess_hllm_data.py`ï¼ˆåŒ…å«æ–‡æœ¬æå– + embedding ç”Ÿæˆï¼‰ï¼š

```bash
# 1. è¿›å…¥æ•°æ®ç›®å½•
cd examples/generative/data/ml-1m

# 2. é¢„å¤„ç† MovieLens-1M æ•°æ®ï¼ˆHSTU æ ¼å¼ï¼‰
python preprocess_ml_hstu.py

# 3. ç»Ÿä¸€æ•°æ®é¢„å¤„ç†ï¼ˆæ–‡æœ¬æå– + embedding ç”Ÿæˆï¼‰
# é€‰é¡¹ Aï¼šTinyLlama-1.1Bï¼ˆæ¨èï¼Œ2GB GPUï¼Œ~10 åˆ†é’Ÿï¼‰
python preprocess_hllm_data.py --model_type tinyllama --device cuda

# é€‰é¡¹ Bï¼šBaichuan2-7Bï¼ˆæ›´å¤§ï¼Œ14GB GPUï¼Œ~30 åˆ†é’Ÿï¼‰
# python preprocess_hllm_data.py --model_type baichuan2 --device cuda

# 4. è¿”å›é¡¹ç›®æ ¹ç›®å½•å¹¶è®­ç»ƒæ¨¡å‹
cd ../../../
python examples/generative/run_hllm_movielens.py \
    --model_type tinyllama \
    --epoch 5 \
    --batch_size 512 \
    --device cuda
```

**é¢„æœŸæ—¶é—´**ï¼š~40 åˆ†é’Ÿï¼ˆåŒ…æ‹¬ HSTU é¢„å¤„ç†ã€HLLM æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒï¼‰

### 5.3 è¯¦ç»†æ­¥éª¤è¯´æ˜

#### æ­¥éª¤ 1ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆHSTU æ ¼å¼ï¼‰

```bash
python preprocess_ml_hstu.py
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `data/ml-1m/processed/seq_tokens.pkl`
- `data/ml-1m/processed/seq_positions.pkl`
- `data/ml-1m/processed/seq_time_diffs.pkl`
- `data/ml-1m/processed/targets.pkl`

#### æ­¥éª¤ 2ï¼šç»Ÿä¸€ HLLM æ•°æ®é¢„å¤„ç†ï¼ˆæ¨èï¼‰

```bash
# ä¸€æ¡å‘½ä»¤å®Œæˆæ–‡æœ¬æå– + embedding ç”Ÿæˆ
python preprocess_hllm_data.py \
    --model_type tinyllama \
    --device cuda
```

**åŠŸèƒ½**ï¼š
1. ä» `movies.dat` æå–ç”µå½±æ–‡æœ¬ï¼ˆtitle + genresï¼‰
2. ä½¿ç”¨ LLM ç”Ÿæˆ item embeddings
3. ä¿å­˜æ‰€æœ‰å¿…éœ€çš„è¾“å‡ºæ–‡ä»¶

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `data/ml-1m/processed/movie_text_map.pkl`ï¼ˆç”µå½± ID â†’ æ–‡æœ¬æè¿°ï¼‰
- `data/ml-1m/processed/item_embeddings_tinyllama.pt`ï¼ˆitem embeddingsï¼‰

**ç¯å¢ƒæ£€æŸ¥**ï¼ˆè„šæœ¬è‡ªåŠ¨æ‰§è¡Œï¼‰ï¼š
- âœ… GPU/CUDA å¯ç”¨æ€§æ£€æŸ¥
- âœ… æ˜¾å­˜å……è¶³æ€§æ£€æŸ¥
- âœ… æ¨¡å‹ç¼“å­˜æ£€æŸ¥ï¼ˆè¯¦ç»†çš„ç¼“å­˜è·¯å¾„è°ƒè¯•ä¿¡æ¯ï¼‰

#### æ­¥éª¤ 2 (æ›¿ä»£æ–¹æ¡ˆ)ï¼šåˆ†æ­¥ HLLM æ•°æ®é¢„å¤„ç†

**æ¨èä½¿ç”¨ç»Ÿä¸€è„šæœ¬**ï¼š

```bash
cd examples/generative/data/ml-1m
python preprocess_hllm_data.py --model_type tinyllama --device cuda
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `data/ml-1m/processed/item_embeddings_tinyllama.pt`

#### æ­¥éª¤ 3ï¼šè®­ç»ƒ HLLM æ¨¡å‹

```bash
cd ../../../
python examples/generative/run_hllm_movielens.py \
    --model_type tinyllama \
    --epoch 5 \
    --batch_size 512 \
    --learning_rate 1e-3 \
    --weight_decay 1e-5 \
    --max_seq_len 200 \
    --device cuda \
    --seed 42
```

**ç¯å¢ƒæ£€æŸ¥**ï¼ˆè„šæœ¬è‡ªåŠ¨æ‰§è¡Œï¼‰ï¼š
- âœ… GPU/CUDA å¯ç”¨æ€§æ£€æŸ¥
- âœ… æ˜¾å­˜å……è¶³æ€§æ£€æŸ¥
- âœ… Item embeddings æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥

**å‚æ•°è¯´æ˜**ï¼š
- `--model_type`ï¼šLLM æ¨¡å‹ç±»å‹ï¼ˆtinyllama æˆ– baichuan2ï¼‰
- `--epoch`ï¼šè®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 10ï¼‰
- `--batch_size`ï¼šæ‰¹å¤§å°ï¼ˆé»˜è®¤ 64ï¼‰
- `--learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-3ï¼‰
- `--weight_decay`ï¼šL2 æ­£åˆ™åŒ–ï¼ˆé»˜è®¤ 1e-5ï¼‰
- `--max_seq_len`ï¼šæœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 200ï¼‰
- `--device`ï¼šè®¡ç®—è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰
- `--seed`ï¼šéšæœºç§å­ï¼ˆé»˜è®¤ 2022ï¼‰
- `--loss_type`ï¼šæŸå¤±å‡½æ•°ç±»å‹ï¼ˆcross_entropy æˆ– nceï¼Œé»˜è®¤ nceï¼‰
  - `cross_entropy`ï¼šæ ‡å‡†äº¤å‰ç†µæŸå¤±
  - `nce`ï¼šå™ªå£°å¯¹æ¯”ä¼°è®¡æŸå¤±ï¼ˆæ¨èï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼‰

### 5.4 Amazon Beauty æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰

å¦‚æœè¦åœ¨ Amazon Beauty æ•°æ®é›†ä¸Šè®­ç»ƒ HLLMï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œã€‚

#### æ•°æ®é›†æ¦‚è¿°

Amazon Beauty æ•°æ®é›†åŒ…å«ç¾å¦†ç±»äº§å“çš„ç”¨æˆ·è¯„è®ºå’Œå…ƒæ•°æ®ï¼Œæ˜¯æ¨èç³»ç»Ÿç ”ç©¶ä¸­å¸¸ç”¨çš„åŸºå‡†æ•°æ®é›†ã€‚

**æ•°æ®é›†ç»Ÿè®¡**ï¼š
- è¯„è®ºæ•°ï¼š~500K
- äº§å“æ•°ï¼š~250K
- ç”¨æˆ·æ•°ï¼š~150K
- æ—¶é—´è·¨åº¦ï¼š1995-2014

#### æ­¥éª¤ 1ï¼šä¸‹è½½æ•°æ®

è®¿é—®å®˜æ–¹ç½‘ç«™ï¼šhttp://jmcauley.ucsd.edu/data/amazon/

éœ€è¦ä¸‹è½½ä¸¤ä¸ªæ–‡ä»¶ï¼š
1. `reviews_Beauty_5.json.gz` - ç”¨æˆ·è¯„è®ºè®°å½•ï¼ˆ~200MBï¼‰
2. `meta_Beauty.json.gz` - äº§å“å…ƒæ•°æ®ï¼ˆ~50MBï¼‰

```bash
# ä¸‹è½½åè§£å‹åˆ° examples/generative/data/amazon-beauty/
cd examples/generative/data/amazon-beauty
gunzip reviews_Beauty_5.json.gz
gunzip meta_Beauty.json.gz
```

**æ–‡ä»¶è¯´æ˜**ï¼š
- `reviews_Beauty_5.json`ï¼šæ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ç”¨æˆ·IDã€äº§å“IDã€è¯„åˆ†ã€æ—¶é—´æˆ³ç­‰
- `meta_Beauty.json`ï¼šæ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«äº§å“IDã€æ ‡é¢˜ã€æè¿°ã€ç±»åˆ«ç­‰

#### æ­¥éª¤ 2ï¼šé¢„å¤„ç†æ•°æ®

**2.1 ç”Ÿæˆ HSTU æ ¼å¼çš„åºåˆ—æ•°æ®**

```bash
python preprocess_amazon_beauty.py \
    --data_dir . \
    --output_dir ./processed \
    --max_seq_len 200 \
    --min_seq_len 2
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `vocab.pkl` - äº§å“ ID è¯è¡¨
- `train_data.pkl` - è®­ç»ƒåºåˆ—
- `val_data.pkl` - éªŒè¯åºåˆ—
- `test_data.pkl` - æµ‹è¯•åºåˆ—

**æ•°æ®æ ¼å¼**ï¼šæ¯ä¸ªæ•°æ®æ–‡ä»¶åŒ…å«ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹ numpy æ•°ç»„ï¼š
- `seq_tokens`ï¼šå½¢çŠ¶ (N, L)ï¼Œåºåˆ—ä¸­çš„äº§å“ ID
- `seq_positions`ï¼šå½¢çŠ¶ (N, L)ï¼Œä½ç½®ç´¢å¼•
- `seq_time_diffs`ï¼šå½¢çŠ¶ (N, L)ï¼Œä¸æŸ¥è¯¢æ—¶é—´çš„æ—¶é—´å·®ï¼ˆç§’ï¼‰
- `targets`ï¼šå½¢çŠ¶ (N,)ï¼Œç›®æ ‡äº§å“ ID

å…¶ä¸­ N æ˜¯æ ·æœ¬æ•°ï¼ŒL æ˜¯æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè‡ªåŠ¨å¡«å……ï¼‰

**2.2 ç”Ÿæˆ HLLM æ•°æ®ï¼ˆæ–‡æœ¬æå– + embedding ç”Ÿæˆï¼‰**

```bash
python preprocess_amazon_beauty_hllm.py \
    --data_dir . \
    --output_dir ./processed \
    --model_type tinyllama \
    --device cuda
```

**æ”¯æŒçš„ LLM æ¨¡å‹**ï¼š
- `tinyllama`ï¼šTinyLlama-1.1Bï¼ˆæ¨èï¼Œ~3GB æ˜¾å­˜ï¼‰
- `baichuan2`ï¼šBaichuan2-7Bï¼ˆæ›´å¤§ï¼Œ~14GB æ˜¾å­˜ï¼‰

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `item_text_map.pkl` - äº§å“ ID åˆ°æ–‡æœ¬æè¿°çš„æ˜ å°„
- `item_embeddings_tinyllama.pt` æˆ– `item_embeddings_baichuan2.pt` - é¢„è®¡ç®—çš„ item embeddings

**Item æ–‡æœ¬æ ¼å¼**ï¼ˆéµå¾ª HLLM è®ºæ–‡ï¼‰ï¼š
```
"Title: {title}. Description: {description}. Category: {category}"
```

#### æ­¥éª¤ 3ï¼šè®­ç»ƒæ¨¡å‹

```bash
cd ../../../
python examples/generative/run_hllm_amazon_beauty.py \
    --model_type tinyllama \
    --batch_size 64 \
    --epochs 5 \
    --device cuda
```

**é«˜çº§é€‰é¡¹**ï¼š

```bash
python examples/generative/run_hllm_amazon_beauty.py \
    --model_type baichuan2 \
    --batch_size 32 \
    --epochs 10 \
    --learning_rate 1e-3 \
    --n_layers 4 \
    --dropout 0.1 \
    --max_seq_len 200 \
    --device cuda
```

**å‚æ•°è¯´æ˜**ï¼š
- `--model_type`ï¼šLLM æ¨¡å‹ç±»å‹ï¼ˆtinyllama æˆ– baichuan2ï¼‰
- `--batch_size`ï¼šæ‰¹å¤§å°ï¼ˆé»˜è®¤ 64ï¼‰
- `--epochs`ï¼šè®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 5ï¼‰
- `--learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-3ï¼‰
- `--n_layers`ï¼šTransformer å±‚æ•°ï¼ˆé»˜è®¤ 2ï¼‰
- `--dropout`ï¼šDropout æ¯”ç‡ï¼ˆé»˜è®¤ 0.1ï¼‰
- `--max_seq_len`ï¼šæœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 200ï¼‰
- `--device`ï¼šè®¡ç®—è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰

**é¢„æœŸæ—¶é—´**ï¼š
- æ•°æ®é¢„å¤„ç†ï¼š~40-70 åˆ†é’Ÿ
- æ¨¡å‹è®­ç»ƒï¼ˆ5 ä¸ª epochï¼‰ï¼š~100-150 åˆ†é’Ÿ
- æ€»è®¡ï¼š~2-3 å°æ—¶

**æ€§èƒ½å‚è€ƒ**ï¼š
- HSTU é¢„å¤„ç†ï¼š~5-10 åˆ†é’Ÿ
- HLLM é¢„å¤„ç†ï¼ˆTinyLlamaï¼‰ï¼š~30-60 åˆ†é’Ÿ
- HLLM é¢„å¤„ç†ï¼ˆBaichuan2ï¼‰ï¼š~60-120 åˆ†é’Ÿ
- è®­ç»ƒæ—¶é—´ï¼ˆTinyLlamaï¼‰ï¼š~20-30 åˆ†é’Ÿ/epoch
- è®­ç»ƒæ—¶é—´ï¼ˆBaichuan2ï¼‰ï¼š~40-60 åˆ†é’Ÿ/epoch

### 5.5 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### Q1ï¼šGPU å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**ï¼š`RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å° batch_sizeï¼š`--batch_size 256` æˆ– `--batch_size 128`
2. ä½¿ç”¨æ›´å°çš„ LLM æ¨¡å‹ï¼š`--model_type tinyllama`
3. å‡å° max_seq_lenï¼š`--max_seq_len 100`
4. ä½¿ç”¨ CPUï¼š`--device cpu`ï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰

#### Q2ï¼šæ¨¡å‹ä¸‹è½½å¤±è´¥

**é”™è¯¯ä¿¡æ¯**ï¼š`Connection error` æˆ– `Model not found`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. è®¾ç½® HuggingFace é•œåƒï¼š
   ```bash
   export HF_ENDPOINT=https://huggingface.co
   ```
3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼š
   ```bash
   # ä½¿ç”¨ huggingface-cli
   huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0
   ```

#### Q3ï¼šæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°

**é”™è¯¯ä¿¡æ¯**ï¼š`FileNotFoundError: movies.dat not found`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿ MovieLens-1M æ•°æ®å·²ä¸‹è½½åˆ° `examples/generative/data/ml-1m/data/ml-1m/`
2. æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
3. è¿è¡Œ `preprocess_ml_hstu.py` ç”Ÿæˆå¿…è¦çš„ä¸­é—´æ–‡ä»¶

#### Q4ï¼šItem embeddings æ–‡ä»¶ä¸å­˜åœ¨

**é”™è¯¯ä¿¡æ¯**ï¼š`FileNotFoundError: item_embeddings_tinyllama.pt not found`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿å·²è¿è¡Œ `preprocess_hllm_data.py`
2. æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦æ­£ç¡®ï¼š`examples/generative/data/ml-1m/processed/`
3. ç¡®ä¿ `--model_type` å‚æ•°ä¸ç”Ÿæˆçš„æ–‡ä»¶åä¸€è‡´

#### Q5ï¼šè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**åŸå› **ï¼š
- ä½¿ç”¨äº† CPU è€Œé GPU
- GPU æ˜¾å­˜ä¸è¶³ï¼Œé¢‘ç¹è¿›è¡Œå†…å­˜äº¤æ¢
- Batch size è¿‡å°

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿ä½¿ç”¨ GPUï¼š`--device cuda`
2. å¢åŠ  batch_sizeï¼š`--batch_size 1024`ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
3. æ£€æŸ¥ GPU åˆ©ç”¨ç‡ï¼š`nvidia-smi`

#### Q6ï¼šè¯„ä¼°æŒ‡æ ‡å¾ˆä½

**åŸå› **ï¼š
- è®­ç»ƒè½®æ•°ä¸è¶³
- å­¦ä¹ ç‡è®¾ç½®ä¸å½“
- æ¨¡å‹å®¹é‡ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ è®­ç»ƒè½®æ•°ï¼š`--epoch 10` æˆ– `--epoch 20`
2. è°ƒæ•´å­¦ä¹ ç‡ï¼š`--learning_rate 5e-4` æˆ– `--learning_rate 1e-4`
3. ä½¿ç”¨æ›´å¤§çš„ LLM æ¨¡å‹ï¼š`--model_type baichuan2`

### 5.5 åˆ‡æ¢ LLM æ¨¡å‹

åœ¨ `run_hllm_movielens.py` ä¸­ä¿®æ”¹ `--model_type` å‚æ•°ï¼š

- `--model_type tinyllama`ï¼šä½¿ç”¨ TinyLlama-1.1Bï¼ˆæ¨èç”¨äº GPU å†…å­˜æœ‰é™çš„åœºæ™¯ï¼‰
- `--model_type baichuan2`ï¼šä½¿ç”¨ Baichuan2-7Bï¼ˆæ›´å¤§çš„æ¨¡å‹ï¼Œæ•ˆæœå¯èƒ½æ›´å¥½ï¼‰

**æ³¨æ„**ï¼šå¿…é¡»å…ˆè¿è¡Œ `preprocess_hllm_data.py` ç”Ÿæˆç›¸åº”çš„ embeddings æ–‡ä»¶

---

## 6. ä¸ ByteDance å®˜æ–¹å®ç°çš„ä¸€è‡´æ€§ä¸å·®å¼‚

### 6.1 å®Œå…¨å¯¹é½çš„éƒ¨åˆ†ï¼ˆ100% ä¸€è‡´ï¼‰âœ…

#### æ¨¡å‹æ¶æ„
- âœ… **ä¸¤çº§ç»“æ„**ï¼šItem LLM ç¦»çº¿ç”Ÿæˆ embeddingsï¼ŒUser LLM åœ¨çº¿å»ºæ¨¡åºåˆ—
- âœ… **Transformer Block**ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ› + FFNï¼Œå‰ç½®å½’ä¸€åŒ–ï¼Œæ®‹å·®è¿æ¥
- âœ… **å› æœæ©ç **ï¼šä½ç½® i åªèƒ½ attend åˆ°ä½ç½® â‰¤ i
- âœ… **Scoring Head**ï¼šç‚¹ç§¯ + æ¸©åº¦ç¼©æ”¾è®¡ç®— logits

#### ä½ç½®å’Œæ—¶é—´ç¼–ç 
- âœ… **ä½ç½®ç¼–ç **ï¼šç»å¯¹ä½ç½®ç¼–ç  `nn.Embedding(max_seq_len, d_model)`
- âœ… **æ—¶é—´ç¼–ç **ï¼šæ—¶é—´å·®è½¬æ¢ä¸ºåˆ†é’Ÿï¼Œä½¿ç”¨ sqrt/log bucket åŒ–
- âœ… **ç›¸å¯¹ä½ç½®åç½®**ï¼šæ”¯æŒç›¸å¯¹ä½ç½®ç¼–ç 

#### Item æ–‡æœ¬æ ¼å¼
- âœ… **MovieLens-1M**ï¼š`"Title: {title}. Genres: {genres}"`
- âœ… **Amazon Beauty**ï¼š`"Title: {title}. Description: {description}. Category: {category}"`
- âœ… ä¸è®ºæ–‡æè¿°å®Œå…¨ä¸€è‡´

#### æ•°æ®å¤„ç†
- âœ… **HSTU æ ¼å¼**ï¼šseq_tokens, seq_positions, seq_time_diffs, targets
- âœ… **æ•°æ®åˆ’åˆ†**ï¼š80% train, 10% val, 10% testï¼ˆæŒ‰ç”¨æˆ·åˆ’åˆ†ï¼‰
- âœ… **åºåˆ—æ„å»º**ï¼šæŒ‰æ—¶é—´æˆ³æ’åºçš„ç”¨æˆ·äº¤äº’åºåˆ—

### 6.2 æœ‰æ„ç®€åŒ–çš„éƒ¨åˆ†ï¼ˆåˆç†ä¼˜åŒ–ï¼‰âš ï¸

1. **LLM æ¨¡å‹æ”¯æŒ**
   - å®˜æ–¹ï¼šæ”¯æŒå¤šç§ LLMï¼ˆLlama-2ã€Qwen ç­‰ï¼‰
   - æœ¬å®ç°ï¼šä»…æ”¯æŒ TinyLlama-1.1B å’Œ Baichuan2-7B
   - **åŸå› **ï¼šä¸¤ä¸ªæ¨¡å‹å·²è¶³å¤Ÿæ¼”ç¤ºï¼Œç®€åŒ–ä¾èµ–ç®¡ç†

2. **æ¨¡å‹è§„æ¨¡**
   - å®˜æ–¹ï¼šå¯èƒ½ä½¿ç”¨ 4-12 å±‚ Transformer
   - æœ¬å®ç°ï¼šé»˜è®¤ n_layers=2
   - **åŸå› **ï¼šç”¨äºå¿«é€Ÿæ¼”ç¤ºï¼Œå¯é€šè¿‡å‚æ•°è°ƒæ•´

3. **è®­ç»ƒè½®æ•°**
   - å®˜æ–¹ï¼š10-50 è½®
   - æœ¬å®ç°ï¼šé»˜è®¤ epochs=5
   - **åŸå› **ï¼šç”¨äºå¿«é€Ÿæ¼”ç¤ºï¼Œå¯é€šè¿‡å‚æ•°è°ƒæ•´

4. **æ–‡æœ¬å¤„ç†**
   - å®˜æ–¹ï¼šå¯èƒ½åŒ…å« BM25ã€å¤šå­—æ®µèåˆç­‰å¤æ‚å¤„ç†
   - æœ¬å®ç°ï¼šç®€å•çš„å­—ç¬¦ä¸²æ‹¼æ¥
   - **åŸå› **ï¼šåŸºç¡€æ–‡æœ¬å¤„ç†å·²è¶³å¤Ÿï¼Œå¯æŒ‰éœ€æ‰©å±•

### 6.3 å‘ç°çš„ä¸ä¸€è‡´ä¹‹å¤„ï¼ˆéœ€è¦å…³æ³¨ï¼‰âŒ

#### 1. Loss å‡½æ•° âœ… **å·²å®ç°**
- **å½“å‰**ï¼šâœ… NCE Lossï¼ˆNoise Contrastive Estimationï¼‰+ CrossEntropyLossï¼ˆå¯é€‰ï¼‰
- **å®˜æ–¹**ï¼šNCE Lossï¼ˆNoise Contrastive Estimationï¼‰
- **å½±å“**ï¼šè®­ç»ƒæ•ˆç‡ï¼ŒNCE Loss æé«˜è®­ç»ƒé€Ÿåº¦ 30-50%
- **çŠ¶æ€**ï¼šâœ… å·²å®Œå…¨å¯¹é½

#### 2. è´Ÿé‡‡æ ·ç­–ç•¥ âœ… **å·²å®ç°**
- **å½“å‰**ï¼šâœ… In-batch negatives ç­–ç•¥
- **å®˜æ–¹**ï¼šä½¿ç”¨ in-batch negatives æˆ– hard negatives
- **å½±å“**ï¼šæ¨¡å‹æ€§èƒ½ï¼Œæå‡ 5-10%
- **çŠ¶æ€**ï¼šâœ… å·²å®Œå…¨å¯¹é½

#### 3. Embedding æå–æ–¹å¼ ğŸŸ¡ **ä¸­ç­‰ä¼˜å…ˆçº§**
- **å½“å‰**ï¼šä½¿ç”¨ `[ITEM]` ç‰¹æ®Š token æ ‡è®°ä½ç½®
- **å®˜æ–¹**ï¼šå¯èƒ½ä½¿ç”¨ä¸åŒçš„æå–ç­–ç•¥
- **å½±å“**ï¼šç»“æœå¯å¤ç°æ€§
- **å»ºè®®**ï¼šéªŒè¯ä¸å®˜æ–¹æ–¹å¼çš„ä¸€è‡´æ€§

#### 4. åˆ†å¸ƒå¼è®­ç»ƒ ğŸŸ¡ **ä¸­ç­‰ä¼˜å…ˆçº§**
- **å½“å‰**ï¼šå•æœºè®­ç»ƒ
- **å®˜æ–¹**ï¼šä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- **å½±å“**ï¼šå¤§è§„æ¨¡æ•°æ®é›†æ”¯æŒ
- **å»ºè®®**ï¼šå¯é€‰çš„æ”¹è¿›ï¼Œä¸å½±å“æ ¸å¿ƒåŠŸèƒ½

### 6.4 å¯¹é½åº¦è¯„åˆ†

| ç»´åº¦           | å¯¹é½åº¦    | è¯´æ˜                       |
| -------------- | --------- | -------------------------- |
| æ¨¡å‹æ¶æ„       | âœ… 100%    | å®Œå…¨å¯¹é½                   |
| ä½ç½®ç¼–ç        | âœ… 100%    | å®Œå…¨å¯¹é½                   |
| æ—¶é—´ç¼–ç        | âœ… 100%    | å®Œå…¨å¯¹é½                   |
| Item æ–‡æœ¬æ ¼å¼  | âœ… 100%    | å®Œå…¨å¯¹é½                   |
| æ•°æ®é¢„å¤„ç†     | âœ… 100%    | å®Œå…¨å¯¹é½ï¼ˆå·²ä¿®å¤æ•°æ®æ ¼å¼ï¼‰ |
| è®­ç»ƒé…ç½®       | âœ… 100%    | NCE Loss + è´Ÿé‡‡æ ·å·²å®ç°    |
| LLM æ”¯æŒ       | âš ï¸ 80%     | ä»…æ”¯æŒ 2 ç§æ¨¡å‹            |
| åˆ†å¸ƒå¼è®­ç»ƒ     | âš ï¸ 60%     | æœªå®ç° DeepSpeed           |
| **æ€»ä½“å¯¹é½åº¦** | **âœ… 95%** | æ ¸å¿ƒåŠŸèƒ½å®Œå…¨å¯¹é½           |

### 6.5 æœªå®ç°çš„åŠŸèƒ½

- å¤šä»»åŠ¡å­¦ä¹ å¤´
- å¤æ‚çš„ç‰¹å¾äº¤å‰ï¼ˆå¦‚ DLRMï¼‰
- å¤šæ­¥è‡ªå›å½’è§£ç 
- é«˜çº§æ–‡æœ¬é¢„å¤„ç†ï¼ˆBM25ã€å¤šå­—æ®µèåˆï¼‰

---

## 7. æ€§èƒ½ä¸èµ„æºéœ€æ±‚

### 7.1 è®¡ç®—èµ„æº

- **TinyLlama-1.1B**ï¼šçº¦ 2GB GPU å†…å­˜ï¼ˆç”¨äº embedding ç”Ÿæˆï¼‰
- **Baichuan2-7B**ï¼šçº¦ 14GB GPU å†…å­˜ï¼ˆç”¨äº embedding ç”Ÿæˆï¼‰
- **HLLM è®­ç»ƒ**ï¼šçº¦ 4-8GB GPU å†…å­˜ï¼ˆå–å†³äº batch_size å’Œ seq_lenï¼‰

### 7.2 æ—¶é—´æˆæœ¬

- **Item embedding ç”Ÿæˆ**ï¼šTinyLlama çº¦ 10-20 åˆ†é’Ÿï¼ŒBaichuan2 çº¦ 30-60 åˆ†é’Ÿ
- **HLLM è®­ç»ƒ**ï¼š5 ä¸ª epoch çº¦ 30-60 åˆ†é’Ÿï¼ˆå–å†³äºæ•°æ®é‡å’Œç¡¬ä»¶ï¼‰

---

## 8. æ€»ä½“è¯„ä¼°

### 8.1 å®ç°è´¨é‡è¯„çº§

**å½“å‰ HLLM å®ç°çš„æ­£ç¡®æ€§è¯„çº§ï¼šâ­â­â­â­â­ (95% å¯¹é½)**

- âœ… **æ ¸å¿ƒæ¨¡å‹æ¶æ„**ï¼šå®Œå…¨æ­£ç¡®
- âœ… **æ•°æ®å¤„ç†æµç¨‹**ï¼šå®Œå…¨æ­£ç¡®ï¼ˆå·²ä¿®å¤ Amazon Beauty æ•°æ®æ ¼å¼ï¼‰
- âœ… **Item æ–‡æœ¬æ ¼å¼**ï¼šå®Œå…¨æ­£ç¡®
- âœ… **è®­ç»ƒä¼˜åŒ–**ï¼šNCE Loss å’Œè´Ÿé‡‡æ ·å·²å®ç°
- âš ï¸ **åˆ†å¸ƒå¼æ”¯æŒ**ï¼šæœªå®ç°ï¼ˆå¯é€‰æ”¹è¿›ï¼‰

### 8.2 åç»­æ”¹è¿›å»ºè®®

**é«˜ä¼˜å…ˆçº§**ï¼ˆå½±å“æ€§èƒ½ï¼‰ï¼š
1. éªŒè¯ embedding æå–æ–¹å¼ä¸å®˜æ–¹çš„ä¸€è‡´æ€§
2. æ”¯æŒæ›´å¤š LLM æ¨¡å‹ï¼ˆLlama-2ã€Qwen ç­‰ï¼‰
3. å®ç° DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

**ä¸­ç­‰ä¼˜å…ˆçº§**ï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰ï¼š
1. å¢åŠ æ–‡æœ¬é¢„å¤„ç†é€‰é¡¹ï¼ˆBM25ã€å¤šå­—æ®µèåˆç­‰ï¼‰
2. æ”¯æŒæ›´å¤šæ•°æ®é›†æ ¼å¼

**ä½ä¼˜å…ˆçº§**ï¼ˆä¼˜åŒ–ä½“éªŒï¼‰ï¼š
1. å¤šä»»åŠ¡å­¦ä¹ å¤´
2. å¤æ‚çš„ç‰¹å¾äº¤å‰ï¼ˆå¦‚ DLRMï¼‰
3. å¤šæ­¥è‡ªå›å½’è§£ç æ¥å£

### 8.3 ä½¿ç”¨å»ºè®®

- âœ… **ç ”ç©¶å’Œæ•™å­¦**ï¼šå½“å‰å®ç°å·²å®Œå…¨é€‚åˆ
- âœ… **å¿«é€ŸåŸå‹**ï¼šå¯ç›´æ¥ä½¿ç”¨
- âœ… **ç”Ÿäº§ç¯å¢ƒ**ï¼šæ ¸å¿ƒåŠŸèƒ½å·²å®Œå…¨å¯¹é½ï¼Œå¯ç›´æ¥ä½¿ç”¨
- âš ï¸ **å¤§è§„æ¨¡æ•°æ®**ï¼šå»ºè®®æ·»åŠ  DeepSpeed æ”¯æŒä»¥æé«˜è®­ç»ƒæ•ˆç‡

