import{_ as i,c as a,o as n,aj as t}from"./chunks/framework.BuEeO6_n.js";const g=JSON.parse('{"title":"ONNX 导出与量化","description":"Torch-RecHub 模型 ONNX 导出与量化","frontmatter":{"title":"ONNX 导出与量化","description":"Torch-RecHub 模型 ONNX 导出与量化"},"headers":[],"relativePath":"zh/serving/onnx.md","filePath":"zh/serving/onnx.md"}'),h={name:"zh/serving/onnx.md"};function e(l,s,p,k,r,o){return n(),a("div",null,[...s[0]||(s[0]=[t(`<h1 id="onnx-导出与量化" tabindex="-1">ONNX 导出与量化 <a class="header-anchor" href="#onnx-导出与量化" aria-label="Permalink to “ONNX 导出与量化”">​</a></h1><p>Torch-RecHub 已支持将训练好的模型导出为 ONNX，用于跨平台推理部署。面向工业推理场景（低延迟、低内存）， <strong>ONNX 导出</strong> 与 <strong>量化（INT8/FP16）</strong> 的完整用法与建议实践。</p><h2 id="安装依赖" tabindex="-1">安装依赖 <a class="header-anchor" href="#安装依赖" aria-label="Permalink to “安装依赖”">​</a></h2><p>ONNX 相关依赖是可选的，按需安装：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;torch-rechub[onnx]&quot;</span></span></code></pre></div><p>说明：</p><ul><li><code>torch-rechub[onnx]</code> 会安装 <code>onnx</code>、<code>onnxruntime</code>，以及用于 FP16 转换的 <code>onnxconverter-common</code>。</li><li>如需 GPU 推理，请自行安装对应 CUDA 版本的 <code>onnxruntime-gpu</code>（与本机 CUDA/驱动匹配）。</li></ul><h2 id="导出-onnx-训练器-export-onnx" tabindex="-1">导出 ONNX（训练器 export_onnx） <a class="header-anchor" href="#导出-onnx-训练器-export-onnx" aria-label="Permalink to “导出 ONNX（训练器 export_onnx）”">​</a></h2><p>Torch-RecHub 各训练器均提供 <code>export_onnx()</code> 方法（CTR/Matching/MTL/Seq），导出过程会自动构造 dummy input，并可支持 <strong>动态 batch size</strong>。</p><h3 id="ctr-排序-精排-导出" tabindex="-1">CTR（排序/精排）导出 <a class="header-anchor" href="#ctr-排序-精排-导出" aria-label="Permalink to “CTR（排序/精排）导出”">​</a></h3><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.trainers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CTRTrainer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># ... trainer.fit(train_dl, val_dl)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer.export_onnx(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;deepfm.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h3 id="matching-召回-导出-全模型-双塔分导" tabindex="-1">Matching（召回）导出：全模型 / 双塔分导 <a class="header-anchor" href="#matching-召回-导出-全模型-双塔分导" aria-label="Permalink to “Matching（召回）导出：全模型 / 双塔分导”">​</a></h3><p>双塔模型通常建议分开导出用户塔与物品塔，线上分别做 embedding 计算：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.trainers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MatchTrainer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 导出用户塔（用于 user embedding）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer.export_onnx(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user_tower.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">mode</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 导出物品塔（用于 item embedding）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer.export_onnx(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;item_tower.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">mode</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;item&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h3 id="多任务-mtl-导出" tabindex="-1">多任务（MTL）导出 <a class="header-anchor" href="#多任务-mtl-导出" aria-label="Permalink to “多任务（MTL）导出”">​</a></h3><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.trainers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MTLTrainer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer.export_onnx(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;mmoe.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h3 id="导出参数与高级控制-onnx-export-kwargs" tabindex="-1">导出参数与高级控制（onnx_export_kwargs） <a class="header-anchor" href="#导出参数与高级控制-onnx-export-kwargs" aria-label="Permalink to “导出参数与高级控制（onnx_export_kwargs）”">​</a></h3><p>如需调整 <code>torch.onnx.export()</code> 的高级参数（例如某些算子导出策略、常量折叠、导出器选择等），可以通过 <code>onnx_export_kwargs</code> 透传：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer.export_onnx(</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;model.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    dynamic_batch</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 动态 batch size（推荐）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    onnx_export_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;do_constant_folding&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # &quot;operator_export_type&quot;: torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # &quot;dynamo&quot;: False,  # 需要动态轴时通常建议关闭（见下文说明）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>导出器选择建议：</p><ul><li><strong>需要动态 batch（或动态序列长度）</strong>：优先使用 legacy 导出（<code>dynamo=False</code>），更稳定地支持 <code>dynamic_axes</code>。</li><li><strong>输入 shape 固定</strong>：可尝试 <code>dynamo=True</code> 获取更好的导出覆盖率（不同 torch 版本表现不同）。</li><li><strong>老版本 PyTorch</strong>：可能不支持 <code>dynamo</code> 参数；Torch-RecHub 会自动兼容（不传该参数）。</li></ul><h3 id="查看-onnx-模型结构" tabindex="-1">查看 ONNX 模型结构 <a class="header-anchor" href="#查看-onnx-模型结构" aria-label="Permalink to “查看 ONNX 模型结构”">​</a></h3><p>导出 ONNX 后，可以使用 <a href="https://netron.app/" target="_blank" rel="noreferrer">Netron</a> 在线查看模型结构：</p><ol><li>打开 <a href="https://netron.app/" target="_blank" rel="noreferrer">https://netron.app/</a></li><li>拖拽或上传导出的 <code>.onnx</code> 文件</li><li>即可可视化查看模型的网络结构、各层参数和张量形状</li></ol><blockquote><p><strong>提示</strong>：Netron 支持多种模型格式（ONNX、TensorFlow、PyTorch 等），是调试和验证导出模型的便捷工具。</p></blockquote><h2 id="onnx-量化-quantization" tabindex="-1">ONNX 量化（Quantization） <a class="header-anchor" href="#onnx-量化-quantization" aria-label="Permalink to “ONNX 量化（Quantization）”">​</a></h2><p>在工业推理中，FP32 往往不是最优解。常用两类压缩方式：</p><ul><li><strong>INT8 动态量化（Dynamic Quantization）</strong>：主要对 Linear/MatMul 等权重做 INT8，通常对 <strong>CPU</strong> 推理加速明显，且精度损失可控。</li><li><strong>FP16 转换</strong>：对支持 Tensor Core 的 GPU 推理更友好，能降低显存占用并提升吞吐。</li></ul><p>Torch-RecHub 在 <code>torch_rechub.utils</code> 提供统一 API：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.utils </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> quantize_model</span></span></code></pre></div><h3 id="_1-int8-动态量化-推荐-cpu" tabindex="-1">1) INT8 动态量化（推荐 CPU） <a class="header-anchor" href="#_1-int8-动态量化-推荐-cpu" aria-label="Permalink to “1) INT8 动态量化（推荐 CPU）”">​</a></h3><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.utils </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> quantize_model</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">quantize_model(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model_fp32.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    output_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model_int8.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    mode</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;int8&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>可选参数（按需）：</p><ul><li><code>per_channel=True</code>：对权重启用 per-channel 量化</li><li><code>reduce_range=True</code>：缩小量化范围（部分 CPU 可能更稳）</li><li><code>weight_type=&quot;qint8&quot;|&quot;quint8&quot;</code>：权重量化类型</li></ul><blockquote><p>注意：不同 <code>onnxruntime</code> 版本对 <code>quantize_dynamic()</code> 的参数支持略有差异；Torch-RecHub 会自动过滤掉当前版本不支持的参数，保证兼容性。</p></blockquote><h3 id="_2-fp16-转换-推荐-gpu" tabindex="-1">2) FP16 转换（推荐 GPU） <a class="header-anchor" href="#_2-fp16-转换-推荐-gpu" aria-label="Permalink to “2) FP16 转换（推荐 GPU）”">​</a></h3><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch_rechub.utils </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> quantize_model</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">quantize_model(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model_fp32.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    output_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model_fp16.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    mode</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;fp16&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    keep_io_types</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 通常建议保留 I/O 为 FP32，兼容性更好</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="脚本示例与基准测试-benchmark" tabindex="-1">脚本示例与基准测试（Benchmark） <a class="header-anchor" href="#脚本示例与基准测试-benchmark" aria-label="Permalink to “脚本示例与基准测试（Benchmark）”">​</a></h2><p>仓库内提供脚本，便于快速验证：</p><h3 id="量化脚本" tabindex="-1">量化脚本 <a class="header-anchor" href="#量化脚本" aria-label="Permalink to “量化脚本”">​</a></h3><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> examples/serving/quantize_onnx.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp32.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_int8.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --mode</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int8</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> examples/serving/quantize_onnx.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp32.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp16.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --mode</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> fp16</span></span></code></pre></div><h3 id="性能对比脚本" tabindex="-1">性能对比脚本 <a class="header-anchor" href="#性能对比脚本" aria-label="Permalink to “性能对比脚本”">​</a></h3><p>对比 FP32 / INT8 / FP16 的 <strong>模型大小</strong> 与 <strong>推理耗时</strong>：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> examples/serving/benchmark_onnx_quantization.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp32</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp32.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --int8</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_int8.onnx</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> examples/serving/benchmark_onnx_quantization.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp32</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp32.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> model_fp16.onnx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --provider</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> CUDAExecutionProvider</span></span></code></pre></div><p>脚本会根据 ONNX 输入签名自动构造 dummy inputs（适合做快速的端到端性能 sanity check）。</p>`,45)])])}const c=i(h,[["render",e]]);export{g as __pageData,c as default};
