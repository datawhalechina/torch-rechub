import{_ as i,c as l,o as a,aj as s}from"./chunks/framework.BuEeO6_n.js";const u=JSON.parse('{"title":"HSTU 模型复现说明","description":"Meta HSTU 模型在 torch-rechub 中的复现说明，包括架构设计和实现细节","frontmatter":{"title":"HSTU 模型复现说明","description":"Meta HSTU 模型在 torch-rechub 中的复现说明，包括架构设计和实现细节"},"headers":[],"relativePath":"zh/blog/hstu_reproduction.md","filePath":"zh/blog/hstu_reproduction.md"}'),t={name:"zh/blog/hstu_reproduction.md"};function o(r,e,n,d,c,h){return a(),l("div",null,[...e[0]||(e[0]=[s(`<h2 id="hstu-模型在-torch-rechub-中的复现说明" tabindex="-1">HSTU 模型在 torch-rechub 中的复现说明 <a class="header-anchor" href="#hstu-模型在-torch-rechub-中的复现说明" aria-label="Permalink to “HSTU 模型在 torch-rechub 中的复现说明”">​</a></h2><p>本文件总结 torch-rechub 中对 Meta HSTU（Hierarchical Sequential Transduction Units）模型的复现情况，重点说明：</p><ul><li>当前实现的整体架构与关键设计细节；</li><li>与 Meta 官方开源实现/论文的一致之处；</li><li>有意简化或仍然存在差异的部分。</li></ul><hr><h2 id="_1-整体架构概览" tabindex="-1">1. 整体架构概览 <a class="header-anchor" href="#_1-整体架构概览" aria-label="Permalink to “1. 整体架构概览”">​</a></h2><h3 id="_1-1-模块划分" tabindex="-1">1.1 模块划分 <a class="header-anchor" href="#_1-1-模块划分" aria-label="Permalink to “1.1 模块划分”">​</a></h3><p>与 HSTU 相关的主要模块如下：</p><ul><li><strong>模型主体</strong>：<code>torch_rechub/models/generative/hstu.py</code><ul><li><code>HSTUModel</code>：Embedding + HSTUBlock + 输出投影</li></ul></li><li><strong>核心层与 Block</strong>：<code>torch_rechub/basic/layers.py</code><ul><li><code>HSTULayer</code>：单层 HSTU 转导单元（多头注意力 + 门控 + FFN）</li><li><code>HSTUBlock</code>：多层 HSTULayer 堆叠</li></ul></li><li><strong>相对位置偏置与词表工具</strong>：<code>torch_rechub/utils/hstu_utils.py</code><ul><li><code>RelPosBias</code>、<code>VocabMask</code>、<code>VocabMapper</code></li></ul></li><li><strong>时间感知数据预处理</strong>：<code>examples/generative/data/ml-1m/preprocess_ml_hstu.py</code></li><li><strong>数据集与数据生成器</strong>：<code>torch_rechub/utils/data.py</code><ul><li><code>SeqDataset</code>、<code>SequenceDataGenerator</code></li></ul></li><li><strong>训练与评估</strong>： <ul><li><code>torch_rechub/trainers/seq_trainer.py</code>：<code>SeqTrainer</code></li><li><code>examples/generative/run_hstu_movielens.py</code>：示例脚本、评估指标</li></ul></li></ul><h3 id="_1-2-数据与任务" tabindex="-1">1.2 数据与任务 <a class="header-anchor" href="#_1-2-数据与任务" aria-label="Permalink to “1.2 数据与任务”">​</a></h3><ul><li>数据集：MovieLens-1M <code>ratings.dat</code>（包含时间戳）</li><li>任务形式：<strong>Next-item prediction</strong>（给定历史序列，预测下一个 item）</li><li>训练目标：自回归式的 next-token 交叉熵损失（仅使用序列最后一个位置的 logits）</li><li>评估指标：HR@K、NDCG@K（K=10, 50, 200）</li></ul><hr><h2 id="_2-hstulayer-与-hstublock-实现细节" tabindex="-1">2. HSTULayer 与 HSTUBlock 实现细节 <a class="header-anchor" href="#_2-hstulayer-与-hstublock-实现细节" aria-label="Permalink to “2. HSTULayer 与 HSTUBlock 实现细节”">​</a></h2><h3 id="_2-1-hstulayer-核心转导单元" tabindex="-1">2.1 HSTULayer：核心转导单元 <a class="header-anchor" href="#_2-1-hstulayer-核心转导单元" aria-label="Permalink to “2.1 HSTULayer：核心转导单元”">​</a></h3><p><code>torch_rechub/basic/layers.py::HSTULayer</code> 实现了论文中的“Sequential Transduction Unit”核心思想：</p><ol><li><p><strong>输入与线性投影</strong></p><ul><li>输入形状：<code>(B, L, D)</code></li><li>通过 <code>proj1: Linear(D → 2·H·dqk + 2·H·dv)</code> 同时产生 Q / K / U / V： <ul><li>Q, K 形状：<code>(B, H, L, dqk)</code></li><li>U, V 形状：<code>(B, H, L, dv)</code></li></ul></li></ul></li><li><p><strong>多头自注意力 + causal mask</strong></p><ul><li>注意力打分：<code>scores = (Q @ K^T) / sqrt(dqk)</code>，形状 <code>(B, H, L, L)</code></li><li>使用严格的 <strong>causal mask</strong>：位置 i 只能看到 <code>≤ i</code> 的 token，防止未来信息泄露。</li><li>可选加上相对位置偏置 <code>RelPosBias</code>。</li><li>softmax 后得到 <code>attn_weights</code>，再与 V 相乘得到 <code>attn_output</code>。</li></ul></li><li><p><strong>门控机制（Gated Attention）</strong></p><ul><li>将注意力输出 <code>attn_output</code> 与门控向量 U 进行逐元素门控：</li><li><code>gated_output = attn_output * sigmoid(U)</code>，形状 <code>(B, L, H·dv)</code>。</li></ul></li><li><p><strong>输出投影与残差 + FFN</strong></p><ul><li>使用 <code>proj2: Linear(H·dv → D)</code> 将多头输出还原到模型维度。</li><li>两个残差块： <ol><li>自注意力 + 门控 + 投影 + Dropout + 残差</li><li>LayerNorm + FFN(4D) + Dropout + 残差</li></ol></li><li>使用 <code>LayerNorm</code> 做 pre-norm，提升深层训练稳定性。</li></ul></li></ol><h3 id="_2-2-hstublock-多层堆叠" tabindex="-1">2.2 HSTUBlock：多层堆叠 <a class="header-anchor" href="#_2-2-hstublock-多层堆叠" aria-label="Permalink to “2.2 HSTUBlock：多层堆叠”">​</a></h3><p><code>HSTUBlock</code> 是多个 <code>HSTULayer</code> 的简单堆叠：</p><ul><li>初始化时构建 <code>n_layers</code> 个 HSTULayer；</li><li>前向传播中按顺序依次传递；</li><li>未做层间不同窗口/不同参数共享的“显式层级结构”，这一点属于对论文中“Hierarchical”概念的工程化简化。</li></ul><p>这一设计与 Meta 官方开源代码的风格一致：通过多层堆叠来实现逐层抽象的“层级”表示，而不是显式的多分辨率分支。</p><hr><h2 id="_3-时间戳建模与时间嵌入" tabindex="-1">3. 时间戳建模与时间嵌入 <a class="header-anchor" href="#_3-时间戳建模与时间嵌入" aria-label="Permalink to “3. 时间戳建模与时间嵌入”">​</a></h2><h3 id="_3-1-数据预处理中的时间差计算" tabindex="-1">3.1 数据预处理中的时间差计算 <a class="header-anchor" href="#_3-1-数据预处理中的时间差计算" aria-label="Permalink to “3.1 数据预处理中的时间差计算”">​</a></h3><p>文件：<code>examples/generative/data/ml-1m/preprocess_ml_hstu.py</code></p><p>核心设计：</p><ul><li>对每个用户的交互序列，使用滑动窗口生成 <code>(history, target)</code> 样本： <ul><li>history = 序列前缀；target = 当前 prefix 之后的一个 item；</li></ul></li><li>对于每个 history，计算 <strong>相对于查询时间的时间差</strong>： <ul><li>查询时间 = history 中最后一个事件的时间戳 <code>query_timestamp</code>；</li><li>对每个历史事件 <code>ts</code>，时间差为 <code>query_timestamp - ts</code>；</li><li>例如时间戳 <code>[100, 200, 300, 400]</code> → 时间差 <code>[300, 200, 100, 0]</code>；</li></ul></li><li>时间差以秒为单位保存为 <code>seq_time_diffs</code>，与 <code>seq_tokens</code> 同长；</li><li>所有序列截断/左侧 padding 到固定长度 <code>max_seq_len</code>，padding 的时间差为 0。</li></ul><p>这与 Meta 官方 HSTU 代码中 <code>query_time - timestamps</code> 的处理方式保持一致，而不是相邻事件时间间隔的形式。</p><h3 id="_3-2-模型中的时间嵌入与-bucket-化" tabindex="-1">3.2 模型中的时间嵌入与 bucket 化 <a class="header-anchor" href="#_3-2-模型中的时间嵌入与-bucket-化" aria-label="Permalink to “3.2 模型中的时间嵌入与 bucket 化”">​</a></h3><p>文件：<code>torch_rechub/models/generative/hstu.py</code></p><ol><li><p><strong>时间嵌入表</strong></p><ul><li><code>self.time_embedding = nn.Embedding(num_time_buckets + 1, d_model, padding_idx=0)</code></li><li>其中 bucket 0 作为 padding bucket。</li></ul></li><li><p><strong>时间差 → bucket 的映射</strong></p></li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 伪代码</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1) 秒 → 分钟</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">minutes </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> time_diffs.float() </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 60.0</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2) 避免 log(0)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">minutes </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clamp(minutes, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">min</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3) 按 sqrt 或 log 映射到 bucket</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;sqrt&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    bucket </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sqrt(minutes)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">elif</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;log&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    bucket </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> log(minutes)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 4) 截断到 [0, num_time_buckets-1]</span></span></code></pre></div><ol start="3"><li><strong>嵌入融合与 Alpha 缩放</strong></li></ol><ul><li>Token Embedding 使用 Alpha 缩放：<code>token_emb = token_embedding(x) * sqrt(d_model)</code>；</li><li>Position Embedding 为标准的绝对位置嵌入；</li><li>Time Embedding 通过上述 bucket 索引查表得到；</li><li>最终序列表示：<code>embeddings = token_emb + pos_emb + time_emb</code>。</li></ul><p>这部分在最近一次提交中完成了对 Meta 官方实现的细节对齐：</p><ul><li>修复了时间差计算方式（由相邻间隔 → 与查询时间差）；</li><li>增加了 <code>/60.0</code> 的时间单位转换；</li><li>增加了 <code>alpha = sqrt(d_model)</code> 的缩放。</li></ul><hr><h2 id="_4-训练与评估流水线" tabindex="-1">4. 训练与评估流水线 <a class="header-anchor" href="#_4-训练与评估流水线" aria-label="Permalink to “4. 训练与评估流水线”">​</a></h2><h3 id="_4-1-seqdataset-与-sequencedatagenerator" tabindex="-1">4.1 SeqDataset 与 SequenceDataGenerator <a class="header-anchor" href="#_4-1-seqdataset-与-sequencedatagenerator" aria-label="Permalink to “4.1 SeqDataset 与 SequenceDataGenerator”">​</a></h3><p>文件：<code>torch_rechub/utils/data.py</code></p><ul><li>近期提交中已<strong>移除旧 3 元组格式的向后兼容逻辑</strong>，统一为 4 元组： <ul><li><code>(seq_tokens, seq_positions, seq_time_diffs, targets)</code>；</li></ul></li><li><code>SeqDataset</code> 负责将 NumPy 数组转换为 PyTorch 张量；</li><li><code>SequenceDataGenerator</code> 根据给定的 train/val/test 划分构造 DataLoader。</li></ul><h3 id="_4-2-seqtrainer-训练与评估" tabindex="-1">4.2 SeqTrainer：训练与评估 <a class="header-anchor" href="#_4-2-seqtrainer-训练与评估" aria-label="Permalink to “4.2 SeqTrainer：训练与评估”">​</a></h3><p>文件：<code>torch_rechub/trainers/seq_trainer.py</code></p><ul><li><code>train_one_epoch</code>： <ul><li>输入 batch 形如：<code>(seq_tokens, seq_positions, seq_time_diffs, targets)</code>；</li><li>将张量移动到设备；</li><li>调用 <code>model(seq_tokens, seq_time_diffs)</code> 得到 <code>(B, L, V)</code> logits；</li><li>只取最后一个位置 <code>logits[:, -1, :]</code> 与 <code>targets</code> 做交叉熵损失；</li></ul></li><li><code>evaluate</code>： <ul><li>与训练阶段类似，同样只使用序列最后一个位置；</li><li>统计平均 loss 与 top-1 准确率，用于早停与模型选择。</li></ul></li></ul><h3 id="_4-3-示例脚本与推荐指标" tabindex="-1">4.3 示例脚本与推荐指标 <a class="header-anchor" href="#_4-3-示例脚本与推荐指标" aria-label="Permalink to “4.3 示例脚本与推荐指标”">​</a></h3><p>文件：<code>examples/generative/run_hstu_movielens.py</code></p><ul><li>负责加载预处理好的 MovieLens 数据（真实数据），构造数据加载器与模型；</li><li>使用 <code>SeqTrainer</code> 进行训练与验证；</li><li><code>evaluate_ranking</code> 函数在测试集上计算 HR@K 与 NDCG@K： <ul><li>模型同样使用最后一个位置的 logits；</li><li>对所有候选 item 排序，计算 top-K 命中率与折损累计增益。</li></ul></li></ul><p>近期在修复时间戳处理逻辑后，测试集指标相比旧实现有显著提升（以 K=10 为例）：</p><ul><li>HR@10：约从 0.17 提升到 0.21+</li><li>NDCG@10：约从 0.08 提升到 0.11+</li></ul><p>这表明时间衰减建模对生成式推荐效果有明显正向作用。</p><hr><h2 id="_5-与-meta-官方实现的一致性与差异" tabindex="-1">5. 与 Meta 官方实现的一致性与差异 <a class="header-anchor" href="#_5-与-meta-官方实现的一致性与差异" aria-label="Permalink to “5. 与 Meta 官方实现的一致性与差异”">​</a></h2><h3 id="_5-1-主要一致点" tabindex="-1">5.1 主要一致点 <a class="header-anchor" href="#_5-1-主要一致点" aria-label="Permalink to “5.1 主要一致点”">​</a></h3><p>与 Meta 官方 HSTU / DLRM-HSTU 实现相比，本框架在以下方面保持较高一致性：</p><ul><li><strong>核心层结构</strong>：HSTULayer 采用 Q/K/V/U 四路线性投影、多头注意力、门控机制与两段残差 FFN，结构上与官方实现高度一致；</li><li><strong>因果掩码</strong>：在注意力打分阶段使用严格的 causal mask，保证生成式任务的因果性；</li><li><strong>时间差定义</strong>：使用 <code>query_time - timestamps</code> 形式的时间差，而非相邻事件间隔；</li><li><strong>时间 bucket 化与嵌入</strong>：支持 sqrt/log 两种 bucket 映射，配合时间嵌入表，与官方思路对齐；</li><li><strong>Alpha 缩放</strong>：对 token embedding 乘以 <code>sqrt(d_model)</code>，与官方实现中的缩放策略一致；</li><li><strong>训练目标</strong>：自回归式的 next-item 交叉熵目标，等价于语言模型式训练。</li></ul><h3 id="_5-2-主要差异与简化" tabindex="-1">5.2 主要差异与简化 <a class="header-anchor" href="#_5-2-主要差异与简化" aria-label="Permalink to “5.2 主要差异与简化”">​</a></h3><p>目前实现仍有以下差异或有意简化：</p><ol><li><p><strong>未包含 DLRM 与多任务头</strong></p><ul><li>官方 DLRM-HSTU 实现支持复杂的特征交叉与多任务学习；</li><li>本框架专注于单任务的 next-item prediction，未实现 DLRM 部分与多目标头。</li></ul></li><li><p><strong>相对位置偏置为简化版本</strong></p><ul><li>当前的 <code>RelPosBias</code> 基于 <code>|i - j|</code> 距离做线性分桶；</li><li>未显式区分方向（正/负距离）、也未使用更复杂的 log-scaling bucket 公式；</li><li>这在工程上更简单，但与官方实现存在细节差异。</li></ul></li><li><p><strong>仅提供单步 next-item 预测接口</strong></p><ul><li>训练和评估阶段都是“给定完整历史 → 预测下一个 item”；</li><li>尚未封装多步自回归解码接口（如 beam search 生成未来 N 步序列）；</li><li>对于大多数推荐 benchmark（只评估下一步）已经足够，但与“通用生成式序列模型”相比功能较少。</li></ul></li><li><p><strong>部分初始化细节不同</strong></p><ul><li>当前使用 <code>xavier_uniform_</code> 初始化大部分线性层和嵌入；</li><li>官方实现中某些嵌入可能使用基于维度的 <code>uniform(-sqrt(1/N), sqrt(1/N))</code>；</li><li>这类初始化差异对最终收敛影响有限，但不是 100% bit-level 复现。</li></ul></li></ol><hr><h2 id="_6-近期提交总结" tabindex="-1">6. 近期提交总结 <a class="header-anchor" href="#_6-近期提交总结" aria-label="Permalink to “6. 近期提交总结”">​</a></h2><ul><li><p>引入了 HSTU 模型、HSTULayer/HSTUBlock、SeqTrainer、SeqDataset 等完整骨架；</p></li><li><p>实现了基本的生成式 next-item 训练与评估流程；</p></li><li><p>时间戳处理、时间嵌入与部分细节尚处于初版实现阶段。</p></li><li><p>重构 MovieLens 预处理脚本：</p><ul><li>使用滑动窗口策略大幅增加训练样本；</li><li>按用户划分 train/val/test，避免数据泄漏；</li><li>正确使用 <code>query_time - timestamps</code> 形式的时间差；</li></ul></li><li><p>修复时间嵌入实现：</p><ul><li>添加秒 → 分钟的时间单位转换；</li><li>增加 <code>alpha = sqrt(d_model)</code> 缩放；</li><li>与官方时间建模逻辑对齐；</li></ul></li><li><p>清理向后兼容逻辑：</p><ul><li>移除 3 元组数据格式，统一为 4 元组 <code>(tokens, positions, time_diffs, targets)</code>；</li><li>简化 SeqDataset、SequenceDataGenerator、SeqTrainer 代码结构；</li></ul></li><li><p>训练与评估结果显示所有排名指标均有显著提升，验证了时间建模修复的必要性和有效性。</p></li></ul><hr><h2 id="_7-小结" tabindex="-1">7. 小结 <a class="header-anchor" href="#_7-小结" aria-label="Permalink to “7. 小结”">​</a></h2><ul><li>当前实现已经在 <strong>HSTU 核心层结构、时间建模与训练目标</strong> 上与 Meta 官方实现高度对齐；</li><li>同时刻意简化了 DLRM、多任务头、复杂特征工程等工程部分，使得该实现更适合作为研究和教学的参考版本；</li><li>如果后续需要进一步逼近“论文级完全复现”，推荐优先完善： <ol><li>RelPosBias 的 bucket 公式与方向建模；</li><li>padding mask 的显式支持；</li><li>多步自回归解码接口与更复杂的下游任务场景。</li></ol></li></ul>`,62)])])}const k=i(t,[["render",o]]);export{u as __pageData,k as default};
