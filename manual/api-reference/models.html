<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Models API Reference | torch-rechub</title>
    <meta name="description" content="Complete API documentation for all models including recall, ranking, and multi-task learning models">
    <meta name="generator" content="VitePress v2.0.0-alpha.15">
    <link rel="preload stylesheet" href="/torch-rechub/assets/style.Dnbe36Wi.css" as="style">
    <link rel="preload stylesheet" href="/torch-rechub/vp-icons.css" as="style">
    
    <script type="module" src="/torch-rechub/assets/app.svTbNx6M.js"></script>
    <link rel="preload" href="/torch-rechub/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/torch-rechub/assets/chunks/theme.DGWjhVpS.js">
    <link rel="modulepreload" href="/torch-rechub/assets/chunks/framework.CFr_S-Bc.js">
    <link rel="modulepreload" href="/torch-rechub/assets/manual_api-reference_models.md.BCvabpY5.lean.js">
    <link rel="icon" href="/torch-rechub/favicon.ico">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-1df9f90f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-331ec75c></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-331ec75c>Skip to content</a><!--]--><!----><header class="VPNav" data-v-1df9f90f data-v-da52a441><div class="VPNavBar" data-v-da52a441 data-v-70946a35><div class="wrapper" data-v-70946a35><div class="container" data-v-70946a35><div class="title" data-v-70946a35><div class="VPNavBarTitle" data-v-70946a35 data-v-1e38c6bc><a class="title" href="/torch-rechub/" data-v-1e38c6bc><!--[--><!--]--><!--[--><img class="VPImage logo" src="/torch-rechub/img/logo.png" alt data-v-8426fc1a><!--]--><span data-v-1e38c6bc>torch-rechub</span><!--[--><!--]--></a></div></div><div class="content" data-v-70946a35><div class="content-body" data-v-70946a35><!--[--><!--]--><div class="VPNavBarSearch search" data-v-70946a35><!--[--><!----><div id="local-search"><button type="button" aria-label="Search" aria-keyshortcuts="/ control+k meta+k" class="DocSearch DocSearch-Button"><span class="DocSearch-Button-Container"><span class="vpi-search DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key"></kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-70946a35 data-v-39714824><span id="main-nav-aria-label" class="visually-hidden" data-v-39714824> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸ  Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/guide/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸš€ Getting Started</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/core/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>âš™ï¸ Core</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/models/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸ° Models</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/tools/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸ› ï¸ Tools</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/serving/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸš€ Serving</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/tutorials/intro.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸ“– Tutorials</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/api/api.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>â„¹ï¸ API</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/torch-rechub/community/faq.html" tabindex="0" data-v-39714824 data-v-52a1d768><!--[--><span data-v-52a1d768>ğŸ‘¥ Community</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-70946a35 data-v-4c1766e2 data-v-42cb505d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-42cb505d><span class="text" data-v-42cb505d><span class="vpi-languages option-icon" data-v-42cb505d></span><!----><span class="vpi-chevron-down text-icon" data-v-42cb505d></span></span></button><div class="menu" data-v-42cb505d><div class="VPMenu" data-v-42cb505d data-v-25a6cce8><!----><!--[--><!--[--><div class="items" data-v-4c1766e2><p class="title" data-v-4c1766e2>English</p><!--[--><div class="VPMenuLink" data-v-4c1766e2 data-v-faf5b206><a class="VPLink link" href="/torch-rechub/zh/manual/api-reference/models.html" lang="zh-CN" data-v-faf5b206><!--[--><span data-v-faf5b206>ä¸­æ–‡</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-70946a35 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-70946a35 data-v-0394ad82 data-v-d07f11e6><!--[--><a class="VPSocialLink no-icon" href="https://github.com/datawhalechina/torch-rechub" aria-label="github" target="_blank" rel="me noopener" data-v-d07f11e6 data-v-591a6b30><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-70946a35 data-v-bf2fac68 data-v-42cb505d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-42cb505d><span class="vpi-more-horizontal icon" data-v-42cb505d></span></button><div class="menu" data-v-42cb505d><div class="VPMenu" data-v-42cb505d data-v-25a6cce8><!----><!--[--><!--[--><div class="group translations" data-v-bf2fac68><p class="trans-title" data-v-bf2fac68>English</p><!--[--><div class="VPMenuLink" data-v-bf2fac68 data-v-faf5b206><a class="VPLink link" href="/torch-rechub/zh/manual/api-reference/models.html" lang="zh-CN" data-v-faf5b206><!--[--><span data-v-faf5b206>ä¸­æ–‡</span><!--]--></a></div><!--]--></div><div class="group" data-v-bf2fac68><div class="item appearance" data-v-bf2fac68><p class="label" data-v-bf2fac68>Appearance</p><div class="appearance-action" data-v-bf2fac68><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bf2fac68 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bf2fac68><div class="item social-links" data-v-bf2fac68><div class="VPSocialLinks social-links-list" data-v-bf2fac68 data-v-d07f11e6><!--[--><a class="VPSocialLink no-icon" href="https://github.com/datawhalechina/torch-rechub" aria-label="github" target="_blank" rel="me noopener" data-v-d07f11e6 data-v-591a6b30><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-70946a35 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-70946a35><div class="divider-line" data-v-70946a35></div></div></div><!----></header><div class="VPLocalNav empty fixed" data-v-1df9f90f data-v-db738f89><div class="container" data-v-db738f89><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-db738f89 data-v-0bf0e06f><button data-v-0bf0e06f>Return to top</button><!----></div></div></div><!----><div class="VPContent" id="VPContent" data-v-1df9f90f data-v-c87f25bf><div class="VPDoc has-aside" data-v-c87f25bf data-v-7011f0d8><!--[--><!--]--><div class="container" data-v-7011f0d8><div class="aside" data-v-7011f0d8><div class="aside-curtain" data-v-7011f0d8></div><div class="aside-container" data-v-7011f0d8><div class="aside-content" data-v-7011f0d8><div class="VPDocAside" data-v-7011f0d8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-60d5052e><div class="content" data-v-60d5052e><div class="outline-marker" data-v-60d5052e></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-60d5052e>On this page</div><ul class="VPDocOutlineItem root" data-v-60d5052e data-v-1ce71065><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-7011f0d8><div class="content-container" data-v-7011f0d8><!--[--><!--]--><main class="main" data-v-7011f0d8><div style="position:relative;" class="vp-doc _torch-rechub_manual_api-reference_models" data-v-7011f0d8><div><h1 id="models-api-reference" tabindex="-1">Models API Reference <a class="header-anchor" href="#models-api-reference" aria-label="Permalink to â€œModels API Referenceâ€">â€‹</a></h1><p>This section provides detailed API documentation for all models in Torch-RecHub.</p><h2 id="recall-models" tabindex="-1">Recall Models <a class="header-anchor" href="#recall-models" aria-label="Permalink to â€œRecall Modelsâ€">â€‹</a></h2><p>Recall models are primarily used in the recall stage for quick retrieval of relevant items from massive candidate sets. They typically adopt two-tower or sequential model structures to meet the efficiency requirements of the recall stage.</p><h3 id="two-tower-model-series" tabindex="-1">Two-Tower Model Series <a class="header-anchor" href="#two-tower-model-series" aria-label="Permalink to â€œTwo-Tower Model Seriesâ€">â€‹</a></h3><h4 id="dssm-deep-structured-semantic-model" tabindex="-1">DSSM (Deep Structured Semantic Model) <a class="header-anchor" href="#dssm-deep-structured-semantic-model" aria-label="Permalink to â€œDSSM (Deep Structured Semantic Model)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: Originally proposed by Microsoft for semantic matching and later widely applied in recommender systems. Adopts classic two-tower structure that separately represents users and items, computing similarity through inner product. This structure allows pre-computation of item vectors during online serving, greatly improving service efficiency. The key lies in learning effective user and item representations.</li><li><strong>Parameters</strong>: <ul><li><code>user_features</code> (list): List of user features</li><li><code>item_features</code> (list): List of item features</li><li><code>hidden_units</code> (list): List of hidden layer units</li><li><code>dropout_rates</code> (list): List of dropout rates</li><li><code>embedding_dim</code> (int): Final representation vector dimension</li></ul></li></ul><h4 id="facebook-dssm" tabindex="-1">Facebook DSSM <a class="header-anchor" href="#facebook-dssm" aria-label="Permalink to â€œFacebook DSSMâ€">â€‹</a></h4><ul><li><strong>Introduction</strong>: Facebook&#39;s improved version of DSSM that incorporates multi-task learning framework. Besides the main recall task, it adds auxiliary tasks to help learn better feature representations. The model can simultaneously optimize multiple related objectives like clicks, favorites, purchases, etc., learning richer user and item representations.</li><li><strong>Parameters</strong>: <ul><li><code>user_features</code> (list): List of user features</li><li><code>item_features</code> (list): List of item features</li><li><code>hidden_units</code> (list): List of hidden layer units</li><li><code>num_tasks</code> (int): Number of tasks</li><li><code>task_types</code> (list): List of task types</li></ul></li></ul><h4 id="youtube-dnn" tabindex="-1">YouTube DNN <a class="header-anchor" href="#youtube-dnn" aria-label="Permalink to â€œYouTube DNNâ€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A deep recall model proposed by YouTube, designed for large-scale video recommendation scenarios. The model aggregates user viewing history through average pooling and combines it with other user features. Innovatively introduces negative sampling techniques and multi-task learning framework to improve training efficiency and effectiveness.</li><li><strong>Parameters</strong>: <ul><li><code>user_features</code> (list): List of user features</li><li><code>item_features</code> (list): List of item features</li><li><code>hidden_units</code> (list): List of hidden layer units</li><li><code>embedding_dim</code> (int): Embedding dimension</li><li><code>max_seq_len</code> (int): Maximum sequence length</li></ul></li></ul><h3 id="sequential-recommendation-series" tabindex="-1">Sequential Recommendation Series <a class="header-anchor" href="#sequential-recommendation-series" aria-label="Permalink to â€œSequential Recommendation Seriesâ€">â€‹</a></h3><h4 id="gru4rec" tabindex="-1">GRU4Rec <a class="header-anchor" href="#gru4rec" aria-label="Permalink to â€œGRU4Recâ€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A pioneering work that first applied GRU networks to session-based sequential recommendation. Through GRU network, it captures temporal dependencies in user behavior sequences, with hidden states at each time step containing information about historical behaviors. The model also introduces special mini-batch construction methods and loss function designs to adapt to the characteristics of sequential recommendation.</li><li><strong>Parameters</strong>: <ul><li><code>item_num</code> (int): Total number of items</li><li><code>hidden_size</code> (int): Size of GRU hidden layer</li><li><code>num_layers</code> (int): Number of GRU layers</li><li><code>dropout_rate</code> (float): Dropout rate</li><li><code>embedding_dim</code> (int): Item embedding dimension</li></ul></li></ul><h4 id="narm-neural-attentive-recommendation-machine" tabindex="-1">NARM (Neural Attentive Recommendation Machine) <a class="header-anchor" href="#narm-neural-attentive-recommendation-machine" aria-label="Permalink to â€œNARM (Neural Attentive Recommendation Machine)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A sequential recommendation model that introduces attention mechanism on top of GRU4Rec. Through attention mechanism, the model can dynamically focus on relevant behaviors in the sequence based on the current prediction target. It maintains both global and local sequence representations, comprehensively capturing user&#39;s short-term interests. This design enables better handling of user interest diversity and dynamics.</li><li><strong>Parameters</strong>: <ul><li><code>item_num</code> (int): Total number of items</li><li><code>hidden_size</code> (int): Size of hidden layer</li><li><code>attention_size</code> (int): Size of attention layer</li><li><code>dropout_rate</code> (float): Dropout rate</li><li><code>embedding_dim</code> (int): Item embedding dimension</li></ul></li></ul><h4 id="sasrec-self-attentive-sequential-recommendation" tabindex="-1">SASRec (Self-Attentive Sequential Recommendation) <a class="header-anchor" href="#sasrec-self-attentive-sequential-recommendation" aria-label="Permalink to â€œSASRec (Self-Attentive Sequential Recommendation)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A representative work that applies Transformer structure to sequential recommendation. Through self-attention mechanism, the model can directly compute and learn relationships between any two behaviors in the sequence, unrestricted by RNN&#39;s inherent sequential dependencies. Position encoding helps preserve temporal information of behaviors, while multi-layer structure allows the model to extract increasingly abstract behavior patterns layer by layer. Compared to RNN-based models, it offers better parallelism and scalability.</li><li><strong>Parameters</strong>: <ul><li><code>item_num</code> (int): Total number of items</li><li><code>max_len</code> (int): Maximum sequence length</li><li><code>num_heads</code> (int): Number of attention heads</li><li><code>num_layers</code> (int): Number of Transformer layers</li><li><code>hidden_size</code> (int): Hidden layer dimension</li><li><code>dropout_rate</code> (float): Dropout rate</li></ul></li></ul><h4 id="mind-multi-interest-network-with-dynamic-routing" tabindex="-1">MIND (Multi-Interest Network with Dynamic routing) <a class="header-anchor" href="#mind-multi-interest-network-with-dynamic-routing" aria-label="Permalink to â€œMIND (Multi-Interest Network with Dynamic routing)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A recall model designed for user&#39;s diverse interests. Through capsule network and dynamic routing mechanism, it extracts multiple interest vectors from user&#39;s behavior sequence. Each interest vector represents user preferences in different aspects, providing a more comprehensive characterization of user interest distribution.</li><li><strong>Parameters</strong>: <ul><li><code>item_num</code> (int): Total number of items</li><li><code>num_interests</code> (int): Number of interest vectors</li><li><code>routing_iterations</code> (int): Number of dynamic routing iterations</li><li><code>hidden_size</code> (int): Hidden layer dimension</li><li><code>embedding_dim</code> (int): Item embedding dimension</li></ul></li></ul><h2 id="ranking-models" tabindex="-1">Ranking Models <a class="header-anchor" href="#ranking-models" aria-label="Permalink to â€œRanking Modelsâ€">â€‹</a></h2><p>Ranking models are primarily used in the fine-ranking stage to precisely rank candidate items. They learn complex interactions between users and items through deep learning methods to generate final ranking scores.</p><h3 id="wide-deep-series" tabindex="-1">Wide &amp; Deep Series <a class="header-anchor" href="#wide-deep-series" aria-label="Permalink to â€œWide &amp; Deep Seriesâ€">â€‹</a></h3><h4 id="widedeep" tabindex="-1">WideDeep <a class="header-anchor" href="#widedeep" aria-label="Permalink to â€œWideDeepâ€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A classic model proposed by Google in 2016 that combines the advantages of linear models and deep neural networks. The Wide part performs memorization through feature crosses, suitable for modeling direct, explicit feature correlations; the Deep part performs generalization through deep networks, capable of learning implicit, high-order feature relationships. This combination allows the model to both memorize historical patterns and generalize to new patterns.</li><li><strong>Parameters</strong>: <ul><li><code>wide_features</code> (list): List of features for the wide part, used in linear layer</li><li><code>deep_features</code> (list): List of features for the deep part, used in deep network</li><li><code>hidden_units</code> (list): List of hidden layer units for the deep network, e.g., [256, 128, 64]</li><li><code>dropout_rates</code> (list): Dropout rates for each layer, used for preventing overfitting</li></ul></li></ul><h4 id="deepfm" tabindex="-1">DeepFM <a class="header-anchor" href="#deepfm" aria-label="Permalink to â€œDeepFMâ€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A model that combines Factorization Machines (FM) feature interactions with deep learning models. The FM part efficiently models second-order feature interactions, while the Deep part learns high-order feature relationships. Compared to Wide&amp;Deep, DeepFM doesn&#39;t require manual feature engineering and can automatically learn feature crosses. The model consists of three parts: first-order features, FM&#39;s second-order interactions, and deep network&#39;s high-order interactions.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li><li><code>dropout_rates</code> (list): List of dropout rates</li><li><code>embedding_dim</code> (int): Feature embedding dimension</li></ul></li></ul><h4 id="dcn-dcn-v2" tabindex="-1">DCN / DCN-V2 <a class="header-anchor" href="#dcn-dcn-v2" aria-label="Permalink to â€œDCN / DCN-V2â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: Learns feature interactions explicitly through specially designed Cross Network layers. Each cross layer performs interactions between feature vectors and their original form, increasing the degree of feature crossing as the depth increases. DCN-V2 improves the cross network parameterization, offering both &quot;vector&quot; and &quot;matrix&quot; options, maintaining model expressiveness while improving efficiency.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>cross_num</code> (int): Number of cross layers</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li><li><code>cross_parameterization</code> (str, DCN-V2): Cross parameterization method, &quot;vector&quot; or &quot;matrix&quot;</li></ul></li></ul><h4 id="afm-attentional-factorization-machine" tabindex="-1">AFM (Attentional Factorization Machine) <a class="header-anchor" href="#afm-attentional-factorization-machine" aria-label="Permalink to â€œAFM (Attentional Factorization Machine)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: Introduces attention mechanism to FM, assigning different importance weights to different feature interactions. Through the attention network, it adaptively learns the importance of feature interactions, identifying feature combinations that are more relevant to the prediction target.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>attention_units</code> (list): Hidden layer units for attention network</li><li><code>embedding_dim</code> (int): Feature embedding dimension</li><li><code>dropout_rate</code> (float): Dropout rate for attention network</li></ul></li></ul><h4 id="fibinet-feature-importance-and-bilinear-feature-interaction-network" tabindex="-1">FiBiNET (Feature Importance and Bilinear feature Interaction Network) <a class="header-anchor" href="#fibinet-feature-importance-and-bilinear-feature-interaction-network" aria-label="Permalink to â€œFiBiNET (Feature Importance and Bilinear feature Interaction Network)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: Dynamically learns feature importance through SENET mechanism and uses bilinear layers for feature interaction. The SENET module helps identify important features, while bilinear interaction provides richer feature interaction methods than inner products.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>bilinear_type</code> (str): Bilinear layer type, options: &quot;field_all&quot;/&quot;field_each&quot;/&quot;field_interaction&quot;</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li><li><code>reduction_ratio</code> (int): Reduction ratio for SENET module</li></ul></li></ul><h3 id="attention-based-series" tabindex="-1">Attention-based Series <a class="header-anchor" href="#attention-based-series" aria-label="Permalink to â€œAttention-based Seriesâ€">â€‹</a></h3><h4 id="din-deep-interest-network" tabindex="-1">DIN (Deep Interest Network) <a class="header-anchor" href="#din-deep-interest-network" aria-label="Permalink to â€œDIN (Deep Interest Network)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A model designed for user interest diversity, using attention mechanism for adaptive learning of user historical behaviors. The model dynamically calculates relevance weights of user historical behaviors based on the current candidate ad, thereby activating relevant user interests and capturing diverse user preferences. It innovatively introduced attention mechanism to recommender systems, pioneering a new paradigm for behavior sequence modeling.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of base features</li><li><code>behavior_features</code> (list): List of behavior features for attention calculation</li><li><code>attention_units</code> (list): Hidden layer units for attention network</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li><li><code>activation</code> (str): Activation function type</li></ul></li></ul><h4 id="dien-deep-interest-evolution-network" tabindex="-1">DIEN (Deep Interest Evolution Network) <a class="header-anchor" href="#dien-deep-interest-evolution-network" aria-label="Permalink to â€œDIEN (Deep Interest Evolution Network)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: An advanced version of DIN that models the dynamic evolution of user interests through interest evolution layer. It uses GRU structure to capture interest evolution and innovatively designs AUGRU (GRU with Attentional Update Gate) to make the interest evolution process aware of target items. It also includes auxiliary loss to supervise the training of interest extraction layer. This design not only captures the dynamic changes of user interests but also models the temporal dependencies of interests.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of base features</li><li><code>behavior_features</code> (list): List of behavior features</li><li><code>interest_units</code> (list): Units for interest extraction layer</li><li><code>gru_type</code> (str): GRU type, &quot;AUGRU&quot; or &quot;AIGRU&quot;</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li></ul></li></ul><h4 id="bst-behavior-sequence-transformer" tabindex="-1">BST (Behavior Sequence Transformer) <a class="header-anchor" href="#bst-behavior-sequence-transformer" aria-label="Permalink to â€œBST (Behavior Sequence Transformer)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A pioneering work that introduces Transformer architecture to recommender systems for modeling user behavior sequences. Through self-attention mechanism, the model can directly compute relationships between any two behaviors in the sequence, overcoming the limitations of RNN models in processing long sequences. Position embedding helps the model perceive temporal information of behaviors, while multi-head attention allows the model to understand user behavior patterns from multiple perspectives.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of base features</li><li><code>behavior_features</code> (list): List of behavior features</li><li><code>num_heads</code> (int): Number of attention heads</li><li><code>num_layers</code> (int): Number of Transformer layers</li><li><code>hidden_size</code> (int): Hidden layer dimension</li><li><code>dropout_rate</code> (float): Dropout rate</li></ul></li></ul><h4 id="edcn-enhancing-explicit-and-implicit-feature-interactions" tabindex="-1">EDCN (Enhancing Explicit and Implicit Feature Interactions) <a class="header-anchor" href="#edcn-enhancing-explicit-and-implicit-feature-interactions" aria-label="Permalink to â€œEDCN (Enhancing Explicit and Implicit Feature Interactions)â€">â€‹</a></h4><ul><li><strong>Introduction</strong>: A deep cross network that enhances both explicit and implicit feature interactions. Through a newly designed cross network structure, it considers both explicit and implicit feature interactions. Introduces gating mechanism to regulate the importance of different orders of feature interactions and uses residual connections to alleviate training issues in deep networks.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>cross_num</code> (int): Number of cross layers</li><li><code>hidden_units</code> (list): Hidden layer units for DNN part</li><li><code>gate_type</code> (str): Gate type, &quot;FGU&quot; or &quot;BGU&quot;</li></ul></li></ul><h2 id="multi-task-models" tabindex="-1">Multi-task Models <a class="header-anchor" href="#multi-task-models" aria-label="Permalink to â€œMulti-task Modelsâ€">â€‹</a></h2><p>Multi-task models learn multiple related tasks jointly to achieve knowledge sharing and transfer, improving overall model performance.</p><h3 id="sharedbottom" tabindex="-1">SharedBottom <a class="header-anchor" href="#sharedbottom" aria-label="Permalink to â€œSharedBottomâ€">â€‹</a></h3><ul><li><strong>Introduction</strong>: The most basic multi-task learning model that shares parameters in bottom network for extracting common feature representations. The shared layers learn common features across tasks, while task-specific layers learn individualized features for each task. This simple yet effective structure laid the foundation for multi-task learning.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>hidden_units</code> (list): Hidden layer units for shared network</li><li><code>task_hidden_units</code> (list): Hidden layer units for task-specific networks</li><li><code>num_tasks</code> (int): Number of tasks</li><li><code>task_types</code> (list): List of task types</li></ul></li></ul><h3 id="esmm-entire-space-multi-task-model" tabindex="-1">ESMM (Entire Space Multi-Task Model) <a class="header-anchor" href="#esmm-entire-space-multi-task-model" aria-label="Permalink to â€œESMM (Entire Space Multi-Task Model)â€">â€‹</a></h3><ul><li><strong>Introduction</strong>: An innovative multi-task model proposed by Alibaba specifically designed to address sample selection bias in recommender systems. Through joint modeling of CVR and CTR tasks, it performs parameter learning in the entire space. The core innovation lies in introducing CTR as an auxiliary task and optimizing CVR estimation through task multiplication relationship. This design not only solves the sample selection bias in traditional CVR estimation but also provides unbiased CTR and CTCVR estimation.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>hidden_units</code> (list): List of hidden layer units</li><li><code>tower_units</code> (list): List of task tower layer units</li><li><code>embedding_dim</code> (int): Feature embedding dimension</li></ul></li></ul><h3 id="mmoe-multi-gate-mixture-of-experts" tabindex="-1">MMoE (Multi-gate Mixture-of-Experts) <a class="header-anchor" href="#mmoe-multi-gate-mixture-of-experts" aria-label="Permalink to â€œMMoE (Multi-gate Mixture-of-Experts)â€">â€‹</a></h3><ul><li><strong>Introduction</strong>: A multi-task learning model proposed by Google that achieves soft parameter sharing through expert mechanism and task-related gating networks. Each expert network can learn specific feature transformations, while gating networks dynamically allocate expert importance for each task. This design allows the model to flexibly combine expert knowledge based on task requirements, effectively handling task differences.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>expert_units</code> (list): Hidden layer units for expert networks</li><li><code>num_experts</code> (int): Number of experts</li><li><code>num_tasks</code> (int): Number of tasks</li><li><code>expert_activation</code> (str): Activation function for expert networks</li><li><code>gate_activation</code> (str): Activation function for gate networks</li></ul></li></ul><h3 id="ple-progressive-layered-extraction" tabindex="-1">PLE (Progressive Layered Extraction) <a class="header-anchor" href="#ple-progressive-layered-extraction" aria-label="Permalink to â€œPLE (Progressive Layered Extraction)â€">â€‹</a></h3><ul><li><strong>Introduction</strong>: An improved version of MMoE that better models task relationships through progressive layered extraction. Introduces the concept of task-specific experts and shared experts, implementing progressive feature extraction through multi-level expert networks. Each layer contains both task-specific experts and shared experts, allowing the model to learn both commonalities and individualities of tasks. This progressive design enhances the model&#39;s ability for knowledge extraction and transfer.</li><li><strong>Parameters</strong>: <ul><li><code>features</code> (list): List of features</li><li><code>expert_units</code> (list): Units for expert networks</li><li><code>num_experts</code> (int): Number of experts per layer</li><li><code>num_layers</code> (int): Number of layers</li><li><code>num_shared_experts</code> (int): Number of shared experts</li><li><code>task_types</code> (list): List of task types</li></ul></li></ul></div></div></main><footer class="VPDocFooter" data-v-7011f0d8 data-v-e257564d><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api_api.md\":\"A-66VmfZ\",\"blog_hllm_reproduction.md\":\"DGw_QFLA\",\"blog_match.md\":\"Dbmtv65x\",\"blog_rank.md\":\"4pCJlY24\",\"cache_api-basic.md\":\"CGpgfakv\",\"cache_api-models.md\":\"CKqTe4Cr\",\"cache_api-trainers.md\":\"l7Op-0s4\",\"cache_api-utils.md\":\"qEvm8nX8\",\"cache_hllm_reproduction.md\":\"SEr1WJjZ\",\"cache_hstu_reproduction.md\":\"Cuqmg-9V\",\"cache_match.md\":\"oRxGdwG1\",\"cache_rank.md\":\"BVMLR97X\",\"cache_å‚è€ƒèµ„æ–™.md\":\"BB8mAuQu\",\"community_changelog.md\":\"BzIj9Nza\",\"community_contributing.md\":\"C4ci_jwy\",\"community_faq.md\":\"CCT-0g7P\",\"contributing.md\":\"B60gEQPy\",\"core_data.md\":\"DhiUH6f5\",\"core_evaluation.md\":\"Tcbby-zM\",\"core_features.md\":\"lJx9Atbz\",\"core_intro.md\":\"BSZDmg9L\",\"guide_install.md\":\"0RXmcXSE\",\"guide_intro.md\":\"B6thu5MX\",\"guide_quick_start.md\":\"fYGiQras\",\"index.md\":\"D6BLudcp\",\"manual_api-reference_basic.md\":\"gix_oEYU\",\"manual_api-reference_models.md\":\"BCvabpY5\",\"manual_api-reference_trainers.md\":\"DeV8vyN9\",\"manual_api-reference_utils.md\":\"ljAtlnqQ\",\"manual_faq.md\":\"D9qYkPgc\",\"manual_getting-started.md\":\"CH5XD2SI\",\"manual_installation.md\":\"DLg97S38\",\"manual_tutorials_matching.md\":\"CD2p4zvc\",\"manual_tutorials_multi-task.md\":\"DWikvf8g\",\"manual_tutorials_ranking.md\":\"B02mimyR\",\"models_generative.md\":\"1stfvmLT\",\"models_intro.md\":\"LruBZ3sW\",\"models_matching.md\":\"CZBP8Phs\",\"models_mtl.md\":\"6jrw2AaX\",\"models_ranking.md\":\"v_X3EvGe\",\"serving_demo.md\":\"Dq1JJMmt\",\"serving_intro.md\":\"lcN1Q8P_\",\"serving_onnx.md\":\"DXHmi9Si\",\"serving_vector_index.md\":\"Bs2N5WRk\",\"tools_callbacks.md\":\"Do70NaKe\",\"tools_intro.md\":\"BHyMM0TD\",\"tools_tracking.md\":\"CeqDASrd\",\"tools_visualization.md\":\"CMTJakOm\",\"tutorials_ctr.md\":\"CbvshbXm\",\"tutorials_intro.md\":\"DH0K3XVg\",\"tutorials_pipeline.md\":\"C6PKGOBe\",\"tutorials_retrieval.md\":\"C-rR3Ycv\",\"zh_api_api.md\":\"BohBm0dc\",\"zh_community_changelog.md\":\"EuH4RmsE\",\"zh_community_contributing.md\":\"p7WeO_SG\",\"zh_community_faq.md\":\"Cdh5kAyr\",\"zh_core_data.md\":\"BmatsmtN\",\"zh_core_evaluation.md\":\"CZ4IY6QN\",\"zh_core_features.md\":\"B3KtgG8c\",\"zh_core_intro.md\":\"CyK4J-gM\",\"zh_guide_install.md\":\"abSbe_hT\",\"zh_guide_intro.md\":\"DUCqvu9j\",\"zh_guide_quick_start.md\":\"CGotv9k1\",\"zh_index.md\":\"FzYY5Fp0\",\"zh_models_generative.md\":\"hl6-S9dV\",\"zh_models_intro.md\":\"qkawg5eg\",\"zh_models_matching.md\":\"CPckUxBI\",\"zh_models_mtl.md\":\"0PMEngFJ\",\"zh_models_ranking.md\":\"DbG-iAX4\",\"zh_serving_demo.md\":\"D96OcQc_\",\"zh_serving_intro.md\":\"C1684zAa\",\"zh_serving_onnx.md\":\"M5gufXv2\",\"zh_serving_vector_index.md\":\"B2v0WA5L\",\"zh_tools_callbacks.md\":\"CuCLKBNf\",\"zh_tools_intro.md\":\"C6LOFaB9\",\"zh_tools_tracking.md\":\"DWCuTSo7\",\"zh_tools_visualization.md\":\"OlgOg9Ly\",\"zh_tutorials_ctr.md\":\"CNQA_2vC\",\"zh_tutorials_intro.md\":\"CfGQ1akf\",\"zh_tutorials_pipeline.md\":\"DA9MvXgk\",\"zh_tutorials_retrieval.md\":\"BhFWK1kx\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"torch-rechub\",\"description\":\"A Lighting Pytorch Framework for Recommendation Models, Easy-to-use and Easy-to-extend.\",\"base\":\"/torch-rechub/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/img/logo.png\",\"search\":{\"provider\":\"local\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/datawhalechina/torch-rechub\"}]},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"themeConfig\":{\"nav\":[{\"text\":\"ğŸ  Home\",\"link\":\"/\"},{\"text\":\"ğŸš€ Getting Started\",\"link\":\"/guide/intro\"},{\"text\":\"âš™ï¸ Core\",\"link\":\"/core/intro\"},{\"text\":\"ğŸ° Models\",\"link\":\"/models/intro\"},{\"text\":\"ğŸ› ï¸ Tools\",\"link\":\"/tools/intro\"},{\"text\":\"ğŸš€ Serving\",\"link\":\"/serving/intro\"},{\"text\":\"ğŸ“– Tutorials\",\"link\":\"/tutorials/intro\"},{\"text\":\"â„¹ï¸ API\",\"link\":\"/api/api\"},{\"text\":\"ğŸ‘¥ Community\",\"link\":\"/community/faq\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"ğŸš€ Getting Started\",\"items\":[{\"text\":\"Overview\",\"link\":\"/guide/intro\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quick Start\",\"link\":\"/guide/quick_start\"}]}],\"/core/\":[{\"text\":\"âš™ï¸ Core Components\",\"items\":[{\"text\":\"Overview\",\"link\":\"/core/intro\"},{\"text\":\"Feature Columns\",\"link\":\"/core/features\"},{\"text\":\"Data Pipeline\",\"link\":\"/core/data\"},{\"text\":\"Training & Eval\",\"link\":\"/core/evaluation\"}]}],\"/models/\":[{\"text\":\"ğŸ° Model Zoo\",\"items\":[{\"text\":\"Overview\",\"link\":\"/models/intro\"},{\"text\":\"Ranking Models\",\"link\":\"/models/ranking\"},{\"text\":\"Matching Models\",\"link\":\"/models/matching\"},{\"text\":\"Multi-Task Models\",\"link\":\"/models/mtl\"},{\"text\":\"Generative Models\",\"link\":\"/models/generative\"}]}],\"/tools/\":[{\"text\":\"ğŸ› ï¸ Dev Tools\",\"items\":[{\"text\":\"Overview\",\"link\":\"/tools/intro\"},{\"text\":\"Visualization\",\"link\":\"/tools/visualization\"},{\"text\":\"Experiment Tracking\",\"link\":\"/tools/tracking\"},{\"text\":\"Callbacks\",\"link\":\"/tools/callbacks\"}]}],\"/serving/\":[{\"text\":\"ğŸš€ Serving\",\"items\":[{\"text\":\"Overview\",\"link\":\"/serving/intro\"},{\"text\":\"ONNX & Quantization\",\"link\":\"/serving/onnx\"},{\"text\":\"Vector Indexing\",\"link\":\"/serving/vector_index\"},{\"text\":\"Serving Demo\",\"link\":\"/serving/demo\"}]}],\"/tutorials/\":[{\"text\":\"ğŸ“– Tutorials\",\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/intro\"},{\"text\":\"CTR Pipeline\",\"link\":\"/tutorials/ctr\"},{\"text\":\"Retrieval System\",\"link\":\"/tutorials/retrieval\"},{\"text\":\"Big Data Pipeline\",\"link\":\"/tutorials/pipeline\"}]}],\"/api/\":[{\"text\":\"â„¹ï¸ API Reference\",\"items\":[{\"text\":\"Main API\",\"link\":\"/api/api\"}]}],\"/community/\":[{\"text\":\"ğŸ“˜ Community\",\"items\":[{\"text\":\"FAQ\",\"link\":\"/community/faq\"},{\"text\":\"Contributing\",\"link\":\"/community/contributing\"},{\"text\":\"Changelog\",\"link\":\"/community/changelog\"}]}]}}},\"zh\":{\"label\":\"ä¸­æ–‡\",\"lang\":\"zh-CN\",\"link\":\"/zh/\",\"themeConfig\":{\"nav\":[{\"text\":\"ğŸ  é¦–é¡µ\",\"link\":\"/zh/\"},{\"text\":\"ğŸš€ å¿«é€Ÿå…¥é—¨\",\"link\":\"/zh/guide/intro\"},{\"text\":\"âš™ï¸ æ ¸å¿ƒç»„ä»¶\",\"link\":\"/zh/core/intro\"},{\"text\":\"ğŸ° æ¨¡å‹åº“\",\"link\":\"/zh/models/intro\"},{\"text\":\"ğŸ› ï¸ ç ”å‘å·¥å…·\",\"link\":\"/zh/tools/intro\"},{\"text\":\"ğŸš€ ç”Ÿäº§éƒ¨ç½²\",\"link\":\"/zh/serving/intro\"},{\"text\":\"ğŸ“– åœºæ™¯æ•™ç¨‹\",\"link\":\"/zh/tutorials/intro\"},{\"text\":\"â„¹ï¸ API\",\"link\":\"/zh/api/api\"},{\"text\":\"ğŸ‘¥ ç¤¾åŒº\",\"link\":\"/zh/community/faq\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"ğŸš€ å¿«é€Ÿå…¥é—¨\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/guide/intro\"},{\"text\":\"å®‰è£…æŒ‡å—\",\"link\":\"/zh/guide/install\"},{\"text\":\"3åˆ†é’Ÿä¸Šæ‰‹\",\"link\":\"/zh/guide/quick_start\"}]}],\"/zh/core/\":[{\"text\":\"âš™ï¸ æ ¸å¿ƒç»„ä»¶\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/core/intro\"},{\"text\":\"ç‰¹å¾å®šä¹‰ (Features)\",\"link\":\"/zh/core/features\"},{\"text\":\"æ•°æ®æµæ°´çº¿ (Data)\",\"link\":\"/zh/core/data\"},{\"text\":\"è®­ç»ƒä¸è¯„ä¼° (Eval)\",\"link\":\"/zh/core/evaluation\"}]}],\"/zh/models/\":[{\"text\":\"ğŸ° æ¨¡å‹åº“\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/models/intro\"},{\"text\":\"æ’åºæ¨¡å‹ (Ranking)\",\"link\":\"/zh/models/ranking\"},{\"text\":\"å¬å›æ¨¡å‹ (Matching)\",\"link\":\"/zh/models/matching\"},{\"text\":\"å¤šä»»åŠ¡æ¨¡å‹ (MTL)\",\"link\":\"/zh/models/mtl\"},{\"text\":\"ç”Ÿæˆå¼æ¨¡å‹ (Generative)\",\"link\":\"/zh/models/generative\"}]}],\"/zh/tools/\":[{\"text\":\"ğŸ› ï¸ ç ”å‘å·¥å…·\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/tools/intro\"},{\"text\":\"å¯è§†åŒ–ç›‘æ§\",\"link\":\"/zh/tools/visualization\"},{\"text\":\"å®éªŒè¿½è¸ª\",\"link\":\"/zh/tools/tracking\"},{\"text\":\"å›è°ƒå‡½æ•°\",\"link\":\"/zh/tools/callbacks\"}]}],\"/zh/serving/\":[{\"text\":\"ğŸš€ ç”Ÿäº§éƒ¨ç½²\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/serving/intro\"},{\"text\":\"ONNX å¯¼å‡ºä¸é‡åŒ–\",\"link\":\"/zh/serving/onnx\"},{\"text\":\"å‘é‡æ£€ç´¢å°è£…\",\"link\":\"/zh/serving/vector_index\"},{\"text\":\"åœ¨çº¿æœåŠ¡ç¤ºä¾‹\",\"link\":\"/zh/serving/demo\"}]}],\"/zh/tutorials/\":[{\"text\":\"ğŸ“– åœºæ™¯æ•™ç¨‹\",\"items\":[{\"text\":\"å¯¼è§ˆ (Overview)\",\"link\":\"/zh/tutorials/intro\"},{\"text\":\"CTR é¢„ä¼°æµç¨‹\",\"link\":\"/zh/tutorials/ctr\"},{\"text\":\"å¬å›ç³»ç»Ÿæ­å»º\",\"link\":\"/zh/tutorials/retrieval\"},{\"text\":\"å…¨é“¾è·¯æµæ°´çº¿\",\"link\":\"/zh/tutorials/pipeline\"}]}],\"/zh/api/\":[{\"text\":\"â„¹ï¸ API Reference\",\"items\":[{\"text\":\"API å‚è€ƒ\",\"link\":\"/zh/api/api\"}]}],\"/zh/community/\":[{\"text\":\"ğŸ“˜ ç¤¾åŒºä¿¡æ¯\",\"items\":[{\"text\":\"å¸¸è§é—®é¢˜ (FAQ)\",\"link\":\"/zh/community/faq\"},{\"text\":\"è´¡çŒ®æŒ‡å— (Contributing)\",\"link\":\"/zh/community/contributing\"},{\"text\":\"ç‰ˆæœ¬æ—¥å¿— (Changelog)\",\"link\":\"/zh/community/changelog\"}]}]}}}},\"scrollOffset\":134,\"cleanUrls\":false,\"additionalConfig\":{}}");</script>
    
  </body>
</html>