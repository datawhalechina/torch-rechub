{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e492bec3",
      "metadata": {},
      "source": [
        "# 02 匹配/召回：DSSM（MovieLens-1M） + Annoy 轻量检索\n",
        "\n",
        "- **目标**：跑通双塔召回：MovieLens-1M 预处理 → `MatchDataGenerator` → `MatchTrainer` 训练 → 导出 user/item tower → 基于 embedding 做轻量 topk 检索（Annoy）。\n",
        "- **数据**：`ml-1m_sample.csv`（仓库内置），跑通后可替换为 `ml-1m.csv`。\n",
        "\n",
        "## MatchTrainer 的 mode 简述\n",
        "- `mode=0`：point-wise / 采样式匹配（更常用的简单设置）\n",
        "- `mode=1`：（按实现）可用于不同的采样/损失设定\n",
        "- `mode=2`：（按实现）可用于更复杂的匹配训练方式\n",
        "\n",
        "注：本教程默认使用 `mode=0`，仅做最小可跑通示例。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d193bcb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATASET_PATH: e:\\RecommendSystemProject\\torch-rechub\\examples\\matching\\data\\ml-1m\\ml-1m_sample.csv\n",
            "SAVE_DIR: e:\\RecommendSystemProject\\torch-rechub\\examples\\matching\\data\\ml-1m\\saved\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch_rechub.basic.features import SparseFeature, SequenceFeature\n",
        "from torch_rechub.models.matching import DSSM\n",
        "from torch_rechub.trainers import MatchTrainer\n",
        "from torch_rechub.utils.data import MatchDataGenerator, df_to_dict\n",
        "from torch_rechub.utils.match import Annoy, gen_model_input, generate_seq_feature_match\n",
        "\n",
        "SEED = 2022\n",
        "DEVICE = \"cpu\"  # 可改为 \"cuda:0\"\n",
        "\n",
        "DATASET_PATH = \"../examples/matching/data/ml-1m/ml-1m_sample.csv\"  # 或 ml-1m.csv\n",
        "SAVE_DIR = \"../examples/matching/data/ml-1m/saved/\"\n",
        "RAW_ID_MAPS_PATH = os.path.join(SAVE_DIR, \"raw_id_maps.npy\")\n",
        "\n",
        "SEQ_MAX_LEN = 50\n",
        "NEG_RATIO = 3\n",
        "MODE = 0\n",
        "\n",
        "EPOCH = 2\n",
        "BATCH_SIZE = 4096\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-6\n",
        "\n",
        "EXPORT_ONNX = False\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(\"DATASET_PATH:\", os.path.abspath(DATASET_PATH))\n",
        "print(\"SAVE_DIR:\", os.path.abspath(SAVE_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2f75d80b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw shape: (100, 10)\n",
            "saved raw_id_maps: ../examples/matching/data/ml-1m/saved/raw_id_maps.npy\n",
            "user_profile: (2, 5) item_profile: (93, 2)\n"
          ]
        }
      ],
      "source": [
        "# 严格对齐 tutorials/Matching.ipynb / examples/matching/run_ml_dssm.py：\n",
        "# - genres -> cate_id（取第一个 genre）\n",
        "# - LabelEncoder +1（0 留给 padding）\n",
        "# - 保存 raw_id_maps.npy\n",
        "\n",
        "data = pd.read_csv(DATASET_PATH)\n",
        "print(\"raw shape:\", data.shape)\n",
        "\n",
        "# genres -> cate_id\n",
        "data[\"cate_id\"] = data[\"genres\"].apply(lambda x: str(x).split(\"|\")[0])\n",
        "\n",
        "sparse_features = [\"user_id\", \"movie_id\", \"gender\", \"age\", \"occupation\", \"zip\", \"cate_id\"]\n",
        "user_col, item_col = \"user_id\", \"movie_id\"\n",
        "\n",
        "feature_max_idx = {}\n",
        "user_map, item_map = None, None\n",
        "\n",
        "for feature in sparse_features:\n",
        "    lbe = LabelEncoder()\n",
        "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
        "    feature_max_idx[feature] = int(data[feature].max() + 1)\n",
        "    if feature == user_col:\n",
        "        user_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}\n",
        "    if feature == item_col:\n",
        "        item_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}\n",
        "\n",
        "np.save(RAW_ID_MAPS_PATH, np.array((user_map, item_map), dtype=object))\n",
        "print(\"saved raw_id_maps:\", RAW_ID_MAPS_PATH)\n",
        "\n",
        "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates(\"user_id\")\n",
        "item_profile = data[[\"movie_id\", \"cate_id\"]].drop_duplicates(\"movie_id\")\n",
        "\n",
        "print(\"user_profile:\", user_profile.shape, \"item_profile:\", item_profile.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "29bd577d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preprocess data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generate sequence features: 100%|██████████| 2/2 [00:00<00:00, 2770.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_train: 384, n_test: 2\n",
            "0 cold start user dropped \n",
            "x_train keys: ['user_id', 'movie_id', 'hist_movie_id', 'histlen_movie_id', 'gender', 'age', 'occupation', 'zip', 'cate_id'] ... len= 9\n",
            "test_user keys: ['user_id', 'movie_id', 'hist_movie_id', 'histlen_movie_id', 'label', 'gender', 'age', 'occupation', 'zip', 'cate_id'] ... len= 10\n",
            "y_train shape: (384,) y_test shape: (2,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 生成训练/测试样本（滑窗 + 负采样）\n",
        "# 注意：generate_seq_feature_match 会生成包含 label 的 df_train/df_test\n",
        "\n",
        "df_train, df_test = generate_seq_feature_match(\n",
        "    data,\n",
        "    user_col=user_col,\n",
        "    item_col=item_col,\n",
        "    time_col=\"timestamp\",\n",
        "    item_attribute_cols=[],\n",
        "    sample_method=1,\n",
        "    mode=MODE,\n",
        "    neg_ratio=NEG_RATIO,\n",
        "    min_item=0,\n",
        ")\n",
        "\n",
        "x_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=SEQ_MAX_LEN)\n",
        "y_train = x_train[\"label\"]\n",
        "x_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=SEQ_MAX_LEN)\n",
        "y_test = x_test[\"label\"]\n",
        "\n",
        "# 训练数据字典里保留 label 以外的字段\n",
        "x_train = {k: v for k, v in x_train.items() if k != \"label\"}\n",
        "\n",
        "all_item = df_to_dict(item_profile)\n",
        "test_user = x_test  # test_user 中仍包含 label/movie_id 等，用于生成 test dataloader & 评估\n",
        "\n",
        "print(\"x_train keys:\", list(x_train.keys())[:10], \"...\", \"len=\", len(x_train))\n",
        "print(\"test_user keys:\", list(test_user.keys())[:10], \"...\", \"len=\", len(test_user))\n",
        "print(\"y_train shape:\", np.asarray(y_train).shape, \"y_test shape:\", np.asarray(y_test).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31a1c267",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 1/1 [00:18<00:00, 18.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 1/1 [00:20<00:00, 20.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inference embedding...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "user inference: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
            "item inference: 100%|██████████| 1/1 [00:04<00:00,  4.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_emb: (2, 64) item_emb: (93, 64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 构造双塔特征（与 examples/matching/run_ml_dssm.py 对齐）\n",
        "\n",
        "user_cols = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
        "item_cols = [\"movie_id\", \"cate_id\"]\n",
        "\n",
        "user_features = [SparseFeature(name, vocab_size=feature_max_idx[name], embed_dim=16) for name in user_cols]\n",
        "user_features += [\n",
        "    SequenceFeature(\n",
        "        \"hist_movie_id\",\n",
        "        vocab_size=feature_max_idx[\"movie_id\"],\n",
        "        embed_dim=16,\n",
        "        pooling=\"mean\",\n",
        "        shared_with=\"movie_id\",\n",
        "    )\n",
        "]\n",
        "\n",
        "item_features = [SparseFeature(name, vocab_size=feature_max_idx[name], embed_dim=16) for name in item_cols]\n",
        "\n",
        "model = DSSM(\n",
        "    user_features,\n",
        "    item_features,\n",
        "    temperature=0.02,\n",
        "    user_params={\"dims\": [128, 64], \"activation\": \"prelu\"},\n",
        "    item_params={\"dims\": [128, 64], \"activation\": \"prelu\"},\n",
        ")\n",
        "\n",
        "trainer = MatchTrainer(\n",
        "    model,\n",
        "    mode=MODE,\n",
        "    optimizer_params={\"lr\": LR, \"weight_decay\": WEIGHT_DECAY},\n",
        "    n_epoch=EPOCH,\n",
        "    device=DEVICE,\n",
        "    model_path=SAVE_DIR,\n",
        ")\n",
        "\n",
        "dg = MatchDataGenerator(x=x_train, y=y_train)\n",
        "# 注意：MatchDataGenerator.generate_dataloader 的签名是 (x_test_user, x_all_item, batch_size, ...)\n",
        "train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=BATCH_SIZE)\n",
        "\n",
        "trainer.fit(train_dl)\n",
        "\n",
        "print(\"inference embedding...\")\n",
        "user_emb = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=SAVE_DIR)\n",
        "item_emb = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=SAVE_DIR)\n",
        "\n",
        "print(\"user_emb:\", tuple(user_emb.shape), \"item_emb:\", tuple(item_emb.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c051fc01",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user(raw)=2 true_item(raw)=434 hit@10=0\n",
            "rec: [np.int64(2194), np.int64(1197), np.int64(3035), np.int64(292), np.int64(3578), np.int64(2916), np.int64(2881), np.int64(1270), np.int64(3071), np.int64(1103)]\n",
            "user(raw)=1 true_item(raw)=48 hit@10=0\n",
            "rec: [np.int64(2194), np.int64(1197), np.int64(3035), np.int64(292), np.int64(3578), np.int64(2916), np.int64(2881), np.int64(1270), np.int64(3071), np.int64(1103)]\n",
            "sample hit@10 over 2 users: 0/2\n"
          ]
        }
      ],
      "source": [
        "# 轻量检索示例：Annoy topk\n",
        "# - 用 item tower embedding 建索引\n",
        "# - 对若干个 user embedding 做 topk query\n",
        "\n",
        "annoy = Annoy(n_trees=10)\n",
        "annoy.fit(item_emb)\n",
        "\n",
        "user_map, item_map = np.load(RAW_ID_MAPS_PATH, allow_pickle=True)\n",
        "\n",
        "TOPK = 10\n",
        "N_SHOW = 5\n",
        "\n",
        "hits = 0\n",
        "for i in range(min(N_SHOW, len(test_user[user_col]))):\n",
        "    uid_enc = int(test_user[user_col][i])\n",
        "    true_item_enc = int(test_user[item_col][i])\n",
        "\n",
        "    idx, scores = annoy.query(v=user_emb[i], n=TOPK)\n",
        "    rec_item_enc = all_item[item_col][idx]\n",
        "\n",
        "    hit = int(true_item_enc in set(rec_item_enc.tolist()))\n",
        "    hits += hit\n",
        "\n",
        "    rec_raw = [item_map[int(x)] for x in rec_item_enc.tolist()]\n",
        "    print(f\"user(raw)={user_map[uid_enc]} true_item(raw)={item_map[true_item_enc]} hit@{TOPK}={hit}\")\n",
        "    print(\"rec:\", rec_raw)\n",
        "\n",
        "print(f\"sample hit@{TOPK} over {min(N_SHOW, len(test_user[user_col]))} users: {hits}/{min(N_SHOW, len(test_user[user_col]))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "81869c61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可选：导出 ONNX（默认关闭）\n",
        "# - 双塔：分别导出 user_tower / item_tower\n",
        "\n",
        "if EXPORT_ONNX:\n",
        "    try:\n",
        "        user_onnx = os.path.join(SAVE_DIR, \"user_tower.onnx\")\n",
        "        item_onnx = os.path.join(SAVE_DIR, \"item_tower.onnx\")\n",
        "        trainer.export_onnx(user_onnx, mode=\"user\")\n",
        "        trainer.export_onnx(item_onnx, mode=\"item\")\n",
        "        print(\"exported:\", user_onnx)\n",
        "        print(\"exported:\", item_onnx)\n",
        "    except Exception as e:\n",
        "        print(\"ONNX export failed:\", repr(e))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
