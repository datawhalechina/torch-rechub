{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch-Rechub Tutorial：Milvus\n",
    "\n",
    "- 场景：召回\n",
    "- 数据：MovieLens-1M\n",
    "\n",
    "- 本教程包括以下内容：\n",
    "    1. 安装并启动milvus服务\n",
    "    2. 使用milvus进行召回\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是milvus?\n",
    "\n",
    "Milvus 是一款云原生向量数据库，它具备高可用、高性能、易拓展的特点，用于海量向量数据的实时召回。\n",
    "\n",
    "Milvus 基于 FAISS、Annoy、HNSW 等向量搜索库构建，核心是解决稠密向量相似度检索的问题。\n",
    "在向量检索库的基础上，Milvus 支持数据分区分片、数据持久化、增量数据摄取、标量向量混合查询、time travel 等功能，\n",
    "同时大幅优化了向量检索的性能，可满足任何向量检索场景的应用需求。\n",
    "通常，建议用户使用 Docker 部署 Milvus，以获得最佳可用性和弹性。\n",
    "\n",
    "Milvus 采用共享存储架构，存储计算完全分离，计算节点支持横向扩展。\n",
    "从架构上来看，Milvus 遵循数据流和控制流分离，整体分为了四个层次，分别为接入层（access layer）、协调服务（coordinator service）、执行节点（worker node）和存储层（storage）。各个层次相互独立，独立扩展和容灾。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前置条件\n",
    "1. 安装docker，可以参考https://www.runoob.com/docker/ubuntu-docker-install.html\n",
    "2. 安装docker-compose，可以参考https://www.runoob.com/docker/docker-compose.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#安装milvus https://milvus.io/docs/install_standalone-docker.md\n",
    "#下载docker-compose配置文件\n",
    "!wget https://github.com/milvus-io/milvus/releases/download/v2.2.2/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "#启动milvus docker镜像\n",
    "!sudo docker-compose up -d\n",
    "#检查milvus状态\n",
    "!sudo docker-compose ps\n",
    "#关闭milvus docker镜像\n",
    "# !sudo docker-compose down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#安装pymilvus\n",
    "!pip install pymilvus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此安装已经完成（本文使用版本为---milvus2.2.2,pymilvus2.2.0），下面我们来使用milvus进行召回。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用milvus进行召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f83656c8210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',1000)\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens数据集\n",
    "- MovieLens数据集是电影网站提供的一份数据集，[原数据](https://grouplens.org/datasets/movielens/1m/)分为三个文件，users.dat movies.dat ratings.dat，包含了用户信息、电影信息和用户对电影的评分信息。\n",
    "\n",
    "- 提供原始数据处理之后（参考examples/matching/data/ml-1m/preprocess_ml.py），全量数据集[**ml-1m.csv**](https://cowtransfer.com/s/5a3ab69ebd314e)\n",
    "\n",
    "- 采样后的**ml-1m_sample.csv**(examples/matching/data/ml-1m/ml-1m_sample.csv)，是在全量数据中取出的前100个样本，调试用。在大家用ml-1m_sample.csv跑通代码后，便可以下载全量数据集测试效果，共100万个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  timestamp                                   title                        genres gender  age  occupation    zip\n",
      "0        1      1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)                         Drama      F    1          10  48067\n",
      "1        1       661       3  978302109        James and the Giant Peach (1996)  Animation|Children's|Musical      F    1          10  48067\n",
      "2        1       914       3  978301968                     My Fair Lady (1964)               Musical|Romance      F    1          10  48067\n",
      "3        1      3408       4  978300275                  Erin Brockovich (2000)                         Drama      F    1          10  48067\n",
      "4        1      2355       5  978824291                    Bug's Life, A (1998)   Animation|Children's|Comedy      F    1          10  48067\n"
     ]
    }
   ],
   "source": [
    "# sample中只有两个用户\n",
    "file_path = '../examples/matching/data/ml-1m/ml-1m_sample.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在MovieLens-1M数据集上数据集训练一个DSSM模型\n",
    "\n",
    "[DSSM 论文链接](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)\n",
    "\n",
    "#### 特征预处理\n",
    "在本DSSM模型中，我们使用两种类别的特征，分别是稀疏特征（SparseFeature）和序列特征（SequenceFeature）。\n",
    "\n",
    "- 对于稀疏特征，是一个离散的、有限的值（例如用户ID，一般会先进行LabelEncoding操作转化为连续整数值），模型将其输入到Embedding层，输出一个Embedding向量。\n",
    "\n",
    "- 对于序列特征，每一个样本是一个List[SparseFeature]（一般是观看历史、搜索历史等），对于这种特征，默认对于每一个元素取Embedding后平均，输出一个Embedding向量。此外，除了平均，还有拼接，最值等方式，可以在pooling参数中指定。\n",
    "\n",
    "- 框架还支持稠密特征（DenseFeature），即一个连续的特征值（例如概率），这种类型一般需归一化处理。但是本样例中未使用。\n",
    "\n",
    "以上三类特征的定义在`torch_rechub/basic/features.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理genres特征，取出其第一个作为标签\n",
    "data[\"cate_id\"] = data[\"genres\"].apply(lambda x: x.split(\"|\")[0])\n",
    "\n",
    "# 指定用户列和物品列的名字、离散和稠密特征，适配框架的接口\n",
    "user_col, item_col = \"user_id\", \"movie_id\"\n",
    "sparse_features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip', \"cate_id\"]\n",
    "dense_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id gender  age  occupation    zip    cate_id\n",
      "0        1      1193      F    1          10  48067      Drama\n",
      "1        1       661      F    1          10  48067  Animation\n",
      "2        1       914      F    1          10  48067    Musical\n",
      "3        1      3408      F    1          10  48067      Drama\n",
      "4        1      2355      F    1          10  48067  Animation\n",
      "LabelEncoding后：\n",
      "   user_id  movie_id  gender  age  occupation  zip  cate_id\n",
      "0        1        32       1    1           1    1        7\n",
      "1        1        17       1    1           1    1        3\n",
      "2        1        22       1    1           1    1        8\n",
      "3        1        91       1    1           1    1        7\n",
      "4        1        66       1    1           1    1        3\n"
     ]
    }
   ],
   "source": [
    "save_dir = '../examples/ranking/data/ml-1m/saved/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# 对SparseFeature进行LabelEncoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(data[sparse_features].head())\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "    if feature == user_col:\n",
    "        user_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}  #encode user id: raw user id\n",
    "    if feature == item_col:\n",
    "        item_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}  #encode item id: raw item id\n",
    "np.save(save_dir+\"raw_id_maps.npy\", (user_map, item_map))  # evaluation时会用到\n",
    "print('LabelEncoding后：')\n",
    "print(data[sparse_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户塔与物品塔\n",
    "在DSSM中，分为用户塔和物品塔，每一个塔的输出是用户/物品的特征拼接后经过MLP（多层感知机）得到的。\n",
    "下面我们来定义物品塔和用户塔都有哪些特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id  gender  age  occupation  zip\n",
      "0         1       1    1           1    1\n",
      "53        2       2    2           2    2\n",
      "   movie_id  cate_id\n",
      "0        32        7\n",
      "1        17        3\n",
      "2        22        8\n",
      "3        91        7\n",
      "4        66        3\n",
      "preprocess data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate sequence features: 100%|██████████| 2/2 [00:00<00:00, 1328.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 384, n_test: 2\n",
      "0 cold start user droped \n",
      "   user_id  movie_id                                      hist_movie_id  histlen_movie_id  label\n",
      "0        2        16        [35, 37, 43, 32, 78, 36, 34, 92, 3, 79, 86]                11      0\n",
      "1        1        18  [87, 51, 25, 41, 65, 53, 91, 34, 74, 32, 5, 18...                29      0\n",
      "2        2        40            [35, 37, 43, 32, 78, 36, 34, 92, 3, 79]                10      0\n",
      "3        1        64  [87, 51, 25, 41, 65, 53, 91, 34, 74, 32, 5, 18...                51      0\n",
      "4        1        39  [87, 51, 25, 41, 65, 53, 91, 34, 74, 32, 5, 18...                34      0\n",
      "{'user_id': array([2, 1, 2]), 'movie_id': array([16, 18, 40]), 'hist_movie_id': array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0, 35, 37, 43, 32, 78, 36, 34, 92,  3,\n",
      "        79, 86],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0, 87, 51, 25, 41, 65, 53, 91, 34, 74, 32,  5,\n",
      "        18, 23, 14, 70, 55, 58, 82, 24, 28, 56, 57,  4, 26, 29, 22, 42,\n",
      "        73, 71],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0, 35, 37, 43, 32, 78, 36, 34, 92,\n",
      "         3, 79]]), 'histlen_movie_id': array([11, 29, 10]), 'label': array([0, 0, 0]), 'gender': array([2, 1, 2]), 'age': array([2, 1, 2]), 'occupation': array([2, 1, 2]), 'zip': array([2, 1, 2]), 'cate_id': array([1, 3, 2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义两个塔对应哪些特征\n",
    "user_cols = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "item_cols = ['movie_id', \"cate_id\"]\n",
    "\n",
    "# 从data中取出相应的数据\n",
    "user_profile = data[user_cols].drop_duplicates('user_id')\n",
    "item_profile = data[item_cols].drop_duplicates('movie_id')\n",
    "print(user_profile.head())\n",
    "print(item_profile.head())\n",
    "from torch_rechub.utils.match import generate_seq_feature_match, gen_model_input\n",
    "df_train, df_test = generate_seq_feature_match(data,\n",
    "                                               user_col,\n",
    "                                               item_col,\n",
    "                                               time_col=\"timestamp\",\n",
    "                                               item_attribute_cols=[],\n",
    "                                               sample_method=1,\n",
    "                                               mode=0,\n",
    "                                               neg_ratio=3,\n",
    "                                               min_item=0)\n",
    "print(df_train.head())\n",
    "x_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "y_train = x_train[\"label\"]\n",
    "x_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "y_test = x_test[\"label\"]\n",
    "print({k: v[:3] for k, v in x_train.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<SparseFeature user_id with Embedding shape (3, 16)>, <SparseFeature gender with Embedding shape (3, 16)>, <SparseFeature age with Embedding shape (3, 16)>, <SparseFeature occupation with Embedding shape (3, 16)>, <SparseFeature zip with Embedding shape (3, 16)>, <SequenceFeature hist_movie_id with Embedding shape (94, 16)>]\n",
      "[<SparseFeature movie_id with Embedding shape (94, 16)>, <SparseFeature cate_id with Embedding shape (11, 16)>]\n"
     ]
    }
   ],
   "source": [
    "#定义特征类型\n",
    "\n",
    "from torch_rechub.basic.features import SparseFeature, SequenceFeature\n",
    "user_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in user_cols\n",
    "]\n",
    "user_features += [\n",
    "    SequenceFeature(\"hist_movie_id\",\n",
    "                    vocab_size=feature_max_idx[\"movie_id\"],\n",
    "                    embed_dim=16,\n",
    "                    pooling=\"mean\",\n",
    "                    shared_with=\"movie_id\")\n",
    "]\n",
    "\n",
    "item_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in item_cols\n",
    "]\n",
    "print(user_features)\n",
    "print(item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_id': array([32, 17, 22]), 'cate_id': array([7, 3, 8])}\n",
      "{'user_id': 2, 'movie_id': 50, 'hist_movie_id': array([ 0,  0,  0,  0, 35, 37, 43, 32, 78, 36, 34, 92,  3, 79, 86, 82, 44,\n",
      "       56, 40, 21, 30, 93, 80, 81, 39, 61, 60, 62, 88, 15, 38, 45, 31, 64,\n",
      "       84, 58, 76, 49, 89, 16, 52, 83,  7, 75, 68, 90,  6, 59,  8, 46]), 'histlen_movie_id': 46, 'label': 1, 'gender': 2, 'age': 2, 'occupation': 2, 'zip': 2, 'cate_id': 1}\n"
     ]
    }
   ],
   "source": [
    "# 将dataframe转为dict\n",
    "from torch_rechub.utils.data import df_to_dict\n",
    "all_item = df_to_dict(item_profile)\n",
    "test_user = x_test\n",
    "print({k: v[:3] for k, v in all_item.items()})\n",
    "print({k: v[0] for k, v in test_user.items()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "- 根据之前的x_train字典和y_train等数据生成训练用的Dataloader（train_dl）、测试用的Dataloader（test_dl, item_dl）。\n",
    "\n",
    "- 定义一个双塔DSSM模型，`user_features`表示用户塔有哪些特征，`user_params`表示用户塔的MLP的各层维度和激活函数。（Note：在这个样例中激活函数的选取对最终结果影响很大，调试时不要修改激活函数的参数）\n",
    "- 定义一个召回训练器MatchTrainer，进行模型的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2/2 [00:00<00:00,  6.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch_rechub.models.matching import DSSM\n",
    "from torch_rechub.trainers import MatchTrainer\n",
    "from torch_rechub.utils.data import MatchDataGenerator\n",
    "# 根据之前处理的数据拿到Dataloader\n",
    "dg = MatchDataGenerator(x=x_train, y=y_train)\n",
    "train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n",
    "\n",
    "# 定义模型\n",
    "model = DSSM(user_features,\n",
    "             item_features,\n",
    "             temperature=0.02,\n",
    "             user_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',  # important!!\n",
    "             },\n",
    "             item_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',  # important!!\n",
    "             })\n",
    "\n",
    "# 模型训练器\n",
    "trainer = MatchTrainer(model,\n",
    "                       mode=0,  # 同上面的mode，需保持一致\n",
    "                       optimizer_params={\n",
    "                           \"lr\": 1e-4,\n",
    "                           \"weight_decay\": 1e-6\n",
    "                       },\n",
    "                       n_epoch=1,\n",
    "                       device='cpu',\n",
    "                       model_path=save_dir)\n",
    "\n",
    "# 开始训练\n",
    "trainer.fit(train_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milvus向量化召回 评估\n",
    "- 使用trainer获取测试集中每个user的embedding和数据集中所有物品的embedding集合\n",
    "- 用annoy构建物品embedding索引，对每个用户向量进行ANN（Approximate Nearest Neighbors）召回K个物品\n",
    "- 查看topk评估指标，一般看recall、precision、hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_rechub.utils.match import Milvus\n",
    "from torch_rechub.basic.metric import topk_metrics\n",
    "\n",
    "def match_evaluation(user_embedding, item_embedding, test_user, all_item, user_col='user_id', item_col='movie_id',\n",
    "                     raw_id_maps=\"./raw_id_maps.npy\", topk=10):\n",
    "    print(\"evaluate embedding matching on test data\")\n",
    "    \n",
    "    milvus = Milvus(dim=64)\n",
    "    milvus.fit(item_embedding)\n",
    "\n",
    "    #for each user of test dataset, get ann search topk result\n",
    "    print(\"matching for topk\")\n",
    "    user_map, item_map = np.load(raw_id_maps, allow_pickle=True)\n",
    "    match_res = collections.defaultdict(dict)  # user id -> predicted item ids\n",
    "    for user_id, user_emb in zip(test_user[user_col], user_embedding):\n",
    "        items_idx, items_scores = milvus.query(v=user_emb, n=topk)  #the index of topk match items\n",
    "        match_res[user_map[user_id]] = np.vectorize(item_map.get)(all_item[item_col][items_idx])\n",
    "\n",
    "    #get ground truth\n",
    "    print(\"generate ground truth\")\n",
    "\n",
    "    data = pd.DataFrame({user_col: test_user[user_col], item_col: test_user[item_col]})\n",
    "    data[user_col] = data[user_col].map(user_map)\n",
    "    data[item_col] = data[item_col].map(item_map)\n",
    "    user_pos_item = data.groupby(user_col).agg(list).reset_index()\n",
    "    ground_truth = dict(zip(user_pos_item[user_col], user_pos_item[item_col]))  # user id -> ground truth\n",
    "\n",
    "    print(\"compute topk metrics\")\n",
    "    out = topk_metrics(y_true=ground_truth, y_pred=match_res, topKs=[topk])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user inference: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "item inference: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate embedding matching on test data\n",
      "Start connecting to Milvus\n",
      "Does collection rechub exist? True\n",
      "Number of entities in Milvus: 93\n",
      "matching for topk\n",
      "generate ground truth\n",
      "compute topk metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2562245/2287123622.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  match_res[user_map[user_id]] = np.vectorize(item_map.get)(all_item[item_col][items_idx])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'NDCG': ['NDCG@10: 0.0'],\n",
       "             'MRR': ['MRR@10: 0.0'],\n",
       "             'Recall': ['Recall@10: 0.0'],\n",
       "             'Hit': ['Hit@10: 0.0'],\n",
       "             'Precision': ['Precision@10: 0.0']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embedding = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=save_dir)\n",
    "item_embedding = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=save_dir)\n",
    "match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=10, raw_id_maps=save_dir+\"raw_id_maps.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1932fa4a71a29435b61e1ecc6cc799c8ac9b937fa969a4a75b18f7cc09e1eea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
