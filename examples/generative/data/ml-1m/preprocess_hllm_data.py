"""Unified HLLM data preprocessing script.

This script combines text extraction and item embedding generation into a single pipeline:
1. Extract movie text information from MovieLens-1M (movies.dat)
2. Generate item embeddings using TinyLlama or Baichuan2
3. Save all necessary output files

Usage:
    python preprocess_hllm_data.py --model_type tinyllama --device cuda
    python preprocess_hllm_data.py --model_type baichuan2 --device cuda
"""

import os
import pickle
import sys

import numpy as np
import pandas as pd
import torch
import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# Get the directory where this script is located
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
_DEFAULT_DATA_DIR = _SCRIPT_DIR
_DEFAULT_OUTPUT_DIR = os.path.join(_SCRIPT_DIR, "processed")

# Official ByteDance HLLM item prompt
ITEM_PROMPT = "Compress the following sentence into embedding: "


def check_environment(model_type, device):
    """Check GPU, CUDA, and VRAM availability."""
    print("\n" + "=" * 80)
    print("ç¯å¢ƒæ£€æŸ¥")
    print("=" * 80)

    if device == 'cuda':
        if not torch.cuda.is_available():
            print("âŒ é”™è¯¯ï¼šæŒ‡å®šäº† --device cudaï¼Œä½†ç³»ç»Ÿä¸­æ²¡æœ‰å¯ç”¨çš„ GPU")
            return False

        print("âœ… GPU å¯ç”¨")
        print(f"   GPU åç§°: {torch.cuda.get_device_name(0)}")

        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   æ€»æ˜¾å­˜: {total_memory:.2f} GB")

        required_vram = 3 if model_type == 'tinyllama' else 16
        if total_memory < required_vram:
            print(f"âš ï¸  è­¦å‘Šï¼šéœ€è¦è‡³å°‘ {required_vram}GB æ˜¾å­˜ï¼Œä½†ç³»ç»Ÿä»…æœ‰ {total_memory:.2f}GB")
            response = input("   æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
            if response != 'y':
                return False
        else:
            print(f"âœ… æ˜¾å­˜å……è¶³ï¼ˆéœ€è¦ {required_vram}GBï¼Œå®é™… {total_memory:.2f}GBï¼‰")
    else:
        print("âœ… ä½¿ç”¨ CPU è¿›è¡Œè®¡ç®—ï¼ˆé€Ÿåº¦ä¼šè¾ƒæ…¢ï¼‰")

    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡\n")
    return True


def check_model_exists(model_type):
    """Check if model exists in HuggingFace cache with detailed debugging."""
    if model_type == 'tinyllama':
        model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
    elif model_type == 'baichuan2':
        model_name = 'baichuan-inc/Baichuan2-7B-Chat'
    else:
        return True

    print(f"\næ£€æŸ¥æ¨¡å‹ç¼“å­˜: {model_name}")

    # Get HuggingFace cache directory
    hf_cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    print(f"   HuggingFace ç¼“å­˜ç›®å½•: {hf_cache_dir}")

    try:
        from huggingface_hub import list_repo_files, try_to_load_from_cache

        # Try to load from cache
        cache_path = try_to_load_from_cache(model_name, revision='main')
        if cache_path is not None:
            print(f"âœ… æ¨¡å‹å·²åœ¨ç¼“å­˜ä¸­: {cache_path}")
            return True
        else:
            print("âš ï¸  æ¨¡å‹æœªåœ¨ç¼“å­˜ä¸­")
    except Exception as e:
        print(f"   ç¼“å­˜æ£€æŸ¥å¼‚å¸¸: {e}")

    # Model not in cache, ask user
    print(f"\nâš ï¸  æ¨¡å‹ {model_name} æœªåœ¨æœ¬åœ°ç¼“å­˜ä¸­")
    print("   é¦–æ¬¡ä½¿ç”¨éœ€è¦ä» HuggingFace ä¸‹è½½ï¼ˆçº¦ 2-7GBï¼‰")
    response = input("   æ˜¯å¦ä» HuggingFace ä¸‹è½½ï¼Ÿ(y/n): ").strip().lower()

    if response != 'y':
        print("âŒ ç”¨æˆ·é€‰æ‹©ä¸ä¸‹è½½æ¨¡å‹ï¼Œé€€å‡º")
        return False

    return True


def extract_movie_text(data_dir, output_dir):
    """Extract movie text information from MovieLens-1M."""
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 1: æå–ç”µå½±æ–‡æœ¬ä¿¡æ¯")
    print("=" * 80)

    movies_file = os.path.join(data_dir, 'movies.dat')
    if not os.path.exists(movies_file):
        print(f"âŒ é”™è¯¯ï¼šç”µå½±æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {movies_file}")
        return None

    print(f"åŠ è½½ç”µå½±å…ƒæ•°æ®: {movies_file}")
    mnames = ['movie_id', 'title', 'genres']
    try:
        movies = pd.read_csv(movies_file, sep='::', header=None, names=mnames, engine='python', encoding='ISO-8859-1')
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–ç”µå½±æ•°æ®æ–‡ä»¶: {e}")
        return None

    print(f"âœ… åŠ è½½äº† {len(movies)} éƒ¨ç”µå½±")

    movie_text_map = {}
    for idx, row in movies.iterrows():
        movie_id = int(row['movie_id'])
        title = str(row['title']).strip()
        genres = str(row['genres']).strip()
        # Official ByteDance HLLM format:
        # "{item_prompt}title: {title}genres: {genres}"
        # Note: Using 'genres' instead of 'tag' for MovieLens dataset
        text = f"{ITEM_PROMPT}title: {title}genres: {genres}"
        movie_text_map[movie_id] = text

        if (idx + 1) % 1000 == 0:
            print(f"  å·²å¤„ç† {idx + 1} / {len(movies)} éƒ¨ç”µå½±")

    print(f"âœ… ç”Ÿæˆäº† {len(movie_text_map)} æ¡æ–‡æœ¬æè¿°")

    # Save text mapping
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'movie_text_map.pkl')
    with open(output_file, 'wb') as f:
        pickle.dump(movie_text_map, f)
    print(f"âœ… æ–‡æœ¬æ˜ å°„å·²ä¿å­˜åˆ°: {output_file}")

    return movie_text_map


def generate_item_embeddings(model_type, movie_text_map, output_dir, device):
    """Generate item embeddings using a pre-trained LLM."""
    print("\n" + "=" * 80)
    print(f"æ­¥éª¤ 2: ç”Ÿæˆ Item Embeddings ({model_type})")
    print("=" * 80)

    if model_type == 'tinyllama':
        model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
        d_model = 2048
    elif model_type == 'baichuan2':
        model_name = 'baichuan-inc/Baichuan2-7B-Chat'
        d_model = 4096
    else:
        raise ValueError(f"Unsupported model_type: {model_type}")

    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if device == 'cuda' else torch.float32, device_map=device, trust_remote_code=True)
    model.eval()

    # Official ByteDance HLLM approach: No special [ITEM] token needed
    # Uses last token's hidden state as item embedding
    print("âœ… ä½¿ç”¨å®˜æ–¹ ByteDance HLLM æ ¼å¼ï¼ˆæœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€ï¼‰")

    # Generate embeddings
    print(f"\nç”Ÿæˆ {len(movie_text_map)} ä¸ª item embeddings...")
    os.makedirs(output_dir, exist_ok=True)

    max_movie_id = max(movie_text_map.keys())
    embeddings_array = np.zeros((max_movie_id + 1, d_model), dtype=np.float32)

    with torch.no_grad():
        for movie_id in tqdm.tqdm(sorted(movie_text_map.keys()), desc="Generating embeddings"):
            text = movie_text_map[movie_id]
            # Text already contains ITEM_PROMPT prefix from extract_movie_text()

            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            outputs = model(**inputs, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]

            # Use last token's hidden state as item embedding
            # This matches official implementation where item_emb_token_n=1
            item_emb = hidden_states[0, -1, :].cpu().numpy()

            embeddings_array[movie_id] = item_emb

    # Save embeddings
    embeddings_tensor = torch.from_numpy(embeddings_array).float()
    output_file = os.path.join(output_dir, f'item_embeddings_{model_type}.pt')
    torch.save(embeddings_tensor, output_file)
    print(f"\nâœ… Item embeddingså·²ä¿å­˜åˆ°: {output_file}")
    print(f"   å½¢çŠ¶: {embeddings_tensor.shape}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Unified HLLM data preprocessing',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python preprocess_hllm_data.py --model_type tinyllama --device cuda
  python preprocess_hllm_data.py --model_type baichuan2 --device cuda
        """
    )
    parser.add_argument('--model_type', default='tinyllama', choices=['tinyllama', 'baichuan2'], help='Type of LLM to use')
    parser.add_argument('--device', default='cuda', choices=['cuda', 'cpu'], help='Device to use')
    parser.add_argument('--data_dir', default=None, help='Directory containing movies.dat')
    parser.add_argument('--output_dir', default=None, help='Directory to save processed files')

    args = parser.parse_args()

    # Use default paths if not provided
    data_dir = args.data_dir if args.data_dir else _DEFAULT_DATA_DIR
    output_dir = args.output_dir if args.output_dir else _DEFAULT_OUTPUT_DIR

    data_dir = os.path.abspath(data_dir)
    output_dir = os.path.abspath(output_dir)

    print(f"\nğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}\n")

    # Check environment
    if not check_environment(args.model_type, args.device):
        sys.exit(1)

    # Check model exists
    if not check_model_exists(args.model_type):
        sys.exit(1)

    # Extract text
    movie_text_map = extract_movie_text(data_dir, output_dir)
    if movie_text_map is None:
        sys.exit(1)

    # Generate embeddings
    generate_item_embeddings(args.model_type, movie_text_map, output_dir, args.device)

    print("\n" + "=" * 80)
    print("âœ… HLLM æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print("\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  - {os.path.join(output_dir, 'movie_text_map.pkl')}")
    print(f"  - {os.path.join(output_dir, f'item_embeddings_{args.model_type}.pt')}")


if __name__ == '__main__':
    main()
